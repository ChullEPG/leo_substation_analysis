{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'red' > Fuzzy C- Means (not needed) <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "def fuzzy_c_means(df, c_values = range(2,11), active_only = False, m=2.0, error=0.005, maxiter=1000):\n",
    "    if active_only:\n",
    "        # Get the feature set - only active power features\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        # Get the feature set - all features\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "\n",
    "    # Create an empty dict to hold cluster assignments for each c value\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each value of c\n",
    "    for c in c_values:\n",
    "        # Apply fuzzy c-means clustering to the feature set\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X.T, c, m, error=error, maxiter=maxiter, init=None)\n",
    "        # Calculate the cluster labels based on the maximum degree of membership\n",
    "        labels = np.argmax(u, axis=0)\n",
    "        # Assign cluster labels to substations\n",
    "        df['cluster_{}'.format(c)] = labels\n",
    "        # Save clustering results\n",
    "        results[c] = labels\n",
    "    \n",
    "    return df, results\n",
    "\n",
    "def fuzzy_c_means(df, c_values=range(2, 11), active_only=False, m=2, error=0.005, maxiter=1000):\n",
    "    if active_only:\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "    \n",
    "    X = X.astype(float)  # Convert X to float dtype\n",
    "    \n",
    "    results = {}\n",
    "    for c in c_values:\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X.T, c, m, error=error, maxiter=maxiter, init=None)\n",
    "        labels = np.argmax(u, axis=0)\n",
    "        df['cluster_{}'.format(c)] = labels\n",
    "        results[c] = labels\n",
    "        \n",
    "    return df, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmeans_results = {}\n",
    "for feature_set in [True, False]:\n",
    "    count = 0\n",
    "    cmeans_results[feature_set] = {}\n",
    "    for k,v in df_dict:\n",
    "        df_dict[k,v], cmeans_results[feature_set][k,v] = fuzzy_c_means(df_dict[k,v], active_only = feature_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'pink'> pk prd features (not used) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak period features </br>\n",
    "\n",
    "No. of peak periods  </br>\n",
    "Occurrence time (starting time) of each peak period </br>\n",
    "Shortest time interval between peaks if more than one peak exists  </br>\n",
    "Duration of each peak  </br>\n",
    "Occurrence time of longest peak period  </br>\n",
    "Duration longest peak period  </br>\n",
    "Upward slope of longest peak  </br>\n",
    "Downward slope of the longest peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_peak_period_features(data, alphabet_size=3, window_size=30, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Identifies peak period features in a time series using Symbolic Aggregate Approximation (SAX).\n",
    "    \n",
    "    Parameters:\n",
    "        - data: a list or numpy array containing the time series data\n",
    "        - alphabet_size: the number of symbols to use in the SAX representation (default is 3)\n",
    "        - window_size: the size of the sliding window to use (default is 30)\n",
    "        - threshold: the threshold value used to identify peak periods (default is 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        - A list of tuples, where each tuple contains the start and end index of a peak period.\n",
    "    \"\"\"\n",
    "    \n",
    "    sax = SAX(wordSize=window_size//alphabet_size, alphabetSize=alphabet_size) # Initialize the SAX object\n",
    "    \n",
    "    # Transform the time series data into a SAX representation\n",
    "    sax_data = sax.to_letter_rep(data)\n",
    "    \n",
    "    # Compute the frequencies of each symbol in the SAX representation\n",
    "    freqs = [sax_data.count(sym) / len(sax_data) for sym in sax.get_symbols()]\n",
    "    \n",
    "    # Find the threshold value for identifying peak periods\n",
    "    mean_freq = sum(freqs) / len(freqs)\n",
    "    std_dev_freq = (sum((f - mean_freq)**2 for f in freqs) / len(freqs))**0.5\n",
    "    peak_threshold = mean_freq + std_dev_freq * threshold\n",
    "    \n",
    "    # Find the indices of the start and end of each peak period\n",
    "    peak_periods = []\n",
    "    in_peak_period = False\n",
    "    for i in range(len(sax_data)):\n",
    "        if freqs[sax_data[i]] >= peak_threshold:\n",
    "            if not in_peak_period:\n",
    "                peak_start = i\n",
    "                in_peak_period = True\n",
    "        else:\n",
    "            if in_peak_period:\n",
    "                peak_periods.append((peak_start * window_size, i * window_size))\n",
    "                in_peak_period = False\n",
    "    \n",
    "    if in_peak_period: # Handle the case where a peak period continues to the end of the data\n",
    "        peak_periods.append((peak_start * window_size, len(data)))\n",
    "    \n",
    "    return peak_periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.piecewise import SymbolicAggregateApproximation\n",
    "import numpy as np\n",
    "\n",
    "# Create a random time series\n",
    "X = np.array(dataframes['76 Church Road']['Active Power [kW]'])\n",
    "\n",
    "# Define the SAX transformation parameters\n",
    "n_bins = 4\n",
    "strategy = 'uniform'\n",
    "window_size = 20\n",
    "\n",
    "# Create the SAX object and transform the time series\n",
    "sax = SymbolicAggregateApproximation(n_segments=5, alphabet_size_avg=n_bins)\n",
    "X_sax = sax.fit_transform(X.reshape(1, -1))\n",
    "\n",
    "# Print the transformed time series\n",
    "print(X_sax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sax(time_series, alphabet_size, n_segments):\n",
    "    # Helper function to convert the time series into symbolic representations using SAX\n",
    "    # Returns the symbolic representations\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(time_series.reshape(-1, 1))\n",
    "    kmeans = KMeans(n_clusters=alphabet_size, random_state=0).fit(scaled_data)\n",
    "    distances = np.min(cdist(scaled_data, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "    thresholds = np.percentile(distances, np.linspace(0, 100, alphabet_size + 1)[1:-1])\n",
    "    symbolic_representation = np.zeros(len(scaled_data))\n",
    "    for i in range(1, alphabet_size):\n",
    "        symbolic_representation[distances <= thresholds[i-1]] = i\n",
    "    symbolic_representation = np.array_split(symbolic_representation, n_segments)\n",
    "    symbolic_representation = [''.join([str(int(symbol)) for symbol in segment]) for segment in symbolic_representation]\n",
    "    return symbolic_representation\n",
    "\n",
    "# First, we need to group the data by day\n",
    "grouped = substation_df.groupby(by=[substation_df['date'].dt.date])\n",
    "\n",
    "# Next, we need to iterate through each day to convert the time series into symbolic representations using SAX\n",
    "peak_periods = []\n",
    "for date, group in grouped:\n",
    "    symbolic_representation = sax(group['Active Power [kW]'].values, 5, 24)\n",
    "    peak_periods.append({\n",
    "        'date': date,\n",
    "        'symbolic_representation': symbolic_representation,\n",
    "    })\n",
    "\n",
    "# Now we can extract the various peak period features from the symbolic representations\n",
    "number_of_peak_periods = []\n",
    "occurrence_time_of_peaks = []\n",
    "shortest_time_interval_between_peaks = []\n",
    "duration_of_peaks = []\n",
    "occurrence_time_of_longest_peak = []\n",
    "duration_longest_peak = []\n",
    "for period in peak_periods:\n",
    "    peaks = [i for i, symbol in enumerate(period['symbolic_representation']) if symbol.count('1') >= 4]\n",
    "    if peaks:\n",
    "        number_of_peak_periods.append(len(peaks))\n",
    "        occurrence_time_of_peaks.append([substation_df[substation_df['date'].dt.date == period['date']].iloc[peak]['date'] for peak in peaks])\n",
    "        shortest_time_interval_between_peaks.append(min\n",
    "####\n",
    "\n",
    "    shortest_interval = None\n",
    "    for i in range(1, len(peaks)):\n",
    "        interval = peaks[i] - peaks[i-1]\n",
    "        if shortest_interval is None or interval < shortest_interval:\n",
    "            shortest_interval = interval\n",
    "    shortest_time_interval_between_peaks.append(shortest_interval)\n",
    "\n",
    "    peak_durations = []\n",
    "    for peak in peaks:\n",
    "        start = peak\n",
    "        end = peak\n",
    "        while end < len(period['symbolic_representation']) - 1 and period['symbolic_representation'][end + 1] == '1':\n",
    "            end += 1\n",
    "        peak_durations.append(end - start + 1)\n",
    "    duration_of_peaks.append(peak_durations)\n",
    "\n",
    "    longest_peak_duration = 0\n",
    "    longest_peak_occurrence = None\n",
    "    for i, duration in enumerate(peak_durations):\n",
    "        if duration > longest_peak_duration:\n",
    "            longest_peak_duration = duration\n",
    "            longest_peak_occurrence = occurrence_time_of_peaks[-1][i]\n",
    "    occurrence_time_of_longest_peak.append(longest_peak_occurrence)\n",
    "    duration_longest_peak.append(longest_peak_duration)\n",
    "\n",
    "# To find the upward and downward slopes of the longest peak, we need to access the original time series data\n",
    "upward_slope_longest_peak = []\n",
    "downward_slope_longest_peak = []\n",
    "for i, period in enumerate(peak_periods):\n",
    "    date = period['date']\n",
    "    group = substation_df[substation_df['date'].dt.date == date]\n",
    "    longest_peak_start = group[group['date'] == occurrence_time_of_longest_peak[i]].index[0]\n",
    "    longest_peak_end = longest_peak_start + duration_longest_peak[i] - 1\n",
    "    longest_peak = group[longest_peak_start:longest_peak_end+1]['Active Power [kW]'].values\n",
    "    upward_slope = (longest_peak[-1] - longest_peak[0]) / duration_longest_peak[i]\n",
    "    downward_slope = (longest_peak[0] - longest_peak[-1]) / duration_longest_peak[i]\n",
    "    upward_slope_longest_peak.append(upward_slope)\n",
    "    downward_slope_longest_peak.append(downward_slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substation_df = df\n",
    "# def find_peaks(power_series):\n",
    "#     # Helper function to find the peaks in a time series\n",
    "#     # Returns a list of indeces of peak hours, where peak is defined as an hour with higher active power than both the previous and next hour  \n",
    "#     peaks = []\n",
    "#     for i in range(1, len(power_series) - 1):\n",
    "#         if power_series[i] > power_series[i-1] and power_series[i] > power_series[i+1]:\n",
    "#             peaks.append(i)\n",
    "#     return peaks\n",
    "\n",
    "# # Group data by day\n",
    "# grouped = substation_df.groupby(by=[substation_df['Date']])\n",
    "\n",
    "# # Next, we need to iterate through each day to find the peak periods\n",
    "# peak_periods = []\n",
    "# for date, group in grouped:\n",
    "#     peaks = find_peaks(group['Active Power [kW]'].values)\n",
    "#     if peaks:\n",
    "#         peak_periods.append({\n",
    "#             'date': date,\n",
    "#             'peaks': peaks,\n",
    "#         })\n",
    "\n",
    "# # Now we can extract the various features for each day\n",
    "# number_of_peak_periods = [len(period['peaks']) for period in peak_periods]\n",
    "# occurrence_time_of_peaks = [substation_df.iloc[period['peaks'][0]]['Date'] for period in peak_periods]\n",
    "# duration_of_peaks = []\n",
    "# longest_peak_duration = []\n",
    "# longest_peak_start = []\n",
    "# longest_peak_end = []\n",
    "# longest_peak_upward_slope = []\n",
    "# longest_peak_downward_slope = []\n",
    "# for period in peak_periods:\n",
    "#     peaks = period['peaks']\n",
    "#     date = period['date']\n",
    "#     peak_durations = [peaks[i+1] - peaks[i] for i in range(len(peaks) - 1)]\n",
    "#     duration_of_peaks.append(peak_durations)\n",
    "#     if peak_durations:\n",
    "#         longest_peak_index = np.argmax(peak_durations)\n",
    "#         longest_peak_start.append(substation_df.iloc[peaks[longest_peak_index]]['Date'])\n",
    "#         longest_peak_end.append(substation_df.iloc[peaks[longest_peak_index + 1]]['Date'])\n",
    "#         longest_peak_duration.append(peak_durations[longest_peak_index])\n",
    "#         longest_peak_values = group['Active Power [kW]'].iloc[peaks[longest_peak_index]:peaks[longest_peak_index + 1] + 1].values\n",
    "#         longest_peak_upward_slope.append(np.polyfit(range(len(longest_peak_values)), longest_peak_values, 1)[0])\n",
    "#         longest_peak_downward_slope.append(np.polyfit(range(len(longest_peak_values[::-1])), longest_peak_values[::-1], 1)[0])\n",
    "#     else:\n",
    "#         longest_peak_duration.append(None)\n",
    "#         longest_peak_start.append(None)\n",
    "#         longest_peak_end.append(None)\n",
    "#         longest_peak_upward_slope.append(None)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# daily_groups =df.groupby(['Date'])\n",
    "# peak_hour = daily_groups['Active Power [kW]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "# peak_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'lightgreen'> GMM Clustering <Font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gmm_clustering(df, n_components_range=range(1, 11), active_only=False):\n",
    "    \n",
    "    if active_only:\n",
    "        # Get the feature set - only active power features\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        # Get the feature set - all features\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "\n",
    "    # Create an empty list to hold the BIC scores\n",
    "    bic_scores = []\n",
    "    \n",
    "    # Create empty dict to hold results\n",
    "    results = {}\n",
    "    \n",
    "    # Loop through each value of n_components\n",
    "    for n_components in n_components_range:\n",
    "        # Fit the GMM model to the feature set\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        gmm.fit(X)\n",
    "        # Calculate the BIC for the clustering\n",
    "        bic_scores.append(gmm.bic(X))\n",
    "        \n",
    "        # Save clustering results\n",
    "        results[n_components] = gmm.predict(X)\n",
    "        \n",
    "        # Assign cluster labels to substations\n",
    "         # Assign cluster labels to substations\n",
    "        df[f'gmm_cluster_{n_components}'] = gmm.predict(X)\n",
    "        \n",
    "    # Plot the BIC scores for each n_components value\n",
    "    plt.plot(n_components_range, bic_scores)\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('BIC')\n",
    "    plt.title('BIC for Gaussian Mixture Models')\n",
    "    plt.show()\n",
    "    \n",
    "    return df, results, bic_scores.index(min(bic_scores)) +1 # +1 because index starts at 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_results = {}\n",
    "bic_scores={}\n",
    "gmm_dict = df_dict.copy()\n",
    "for feature_set in [True, False]:\n",
    "    count = 0\n",
    "    gmm_results[feature_set] = {}\n",
    "    bic_scores[feature_set]={}\n",
    "    for k,v in gmm_dict:\n",
    "        print(k,v)\n",
    "        gmm_dict[k,v], gmm_results[feature_set][k,v], bic_scores[feature_set][k,v] = gmm_clustering(df_dict[k,v], active_only = feature_set)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_dict[k,v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = True\n",
    "for k,v in results[feature_set]:\n",
    "    print(k,v )\n",
    "    print(optimal_bic)\n",
    "    optimal_bic = bic_scores[feature_set][k,v]\n",
    "    this_df=  pd.DataFrame(gmm_dict[k,v])\n",
    "    this_df = this_df.loc[:, this_df.columns.str.contains(f'gmm_cluster_{optimal_bic}|substation')]\n",
    "    this_df.to_csv(f'gmm_results/{k}_{v}_gmm.csv', index=False)\n",
    "    \n",
    "  #  pd.DataFrame(results[feature_set][k,v][optimal_bic]).to_csv(f'gmm_results/{k}_{v}_gmm.csv', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
