{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.signal import stft \n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_score\n",
    "import statistics\n",
    "import saxpy \n",
    "from IFEEL import ifeel_transformation, ifeel_extraction\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from skfuzzy.cluster import cmeans as FuzzyCMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.spatial.distance import cdist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"Substation_Data/\"\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Extract the substation name from the file name\n",
    "        substation_name = file_name.split(\" \")[2:]\n",
    "        substation_name = substation_name[:substation_name.index(\"POWER\")]\n",
    "        substation_name = \" \".join(substation_name)\n",
    "        \n",
    "        # Read the CSV file into a pandas dataframe\n",
    "        df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "        # Convert date column to datetime -- using this as opportunity to skip over empty dataframes (if you don't want this, add \"errors = 'ignore'\" to the to_datetime command)\n",
    "        try:\n",
    "            df['Datetime'] = pd.to_datetime(df['Date (Dublin. Edinburgh. Lisbon. London)']) \n",
    "        except:\n",
    "            continue\n",
    "        # Drop old date column \n",
    "        df.drop('Date (Dublin. Edinburgh. Lisbon. London)', axis=1, inplace=True)\n",
    "        # Add the dataframe to the dictionary with the substation ID as the key\n",
    "        dataframes[substation_name] = df\n",
    "\n",
    "# Print the dataframe keys to check that the IDs processed appropriately\n",
    "print(dataframes.keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19/01/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"mean\", \"max\", \"min\", \"std\", 'median', 'var', 'sum']\n",
    "dict_Features = {\"Active Power [kW]\": pd.DataFrame(columns=columns, index = dataframes.keys()),\n",
    "               \"Reactive Power [kVAr]\": pd.DataFrame(columns=columns, index = dataframes.keys()),\n",
    "               \"Apparent Power [kVA]\": pd.DataFrame(columns=columns, index = dataframes.keys()),\n",
    "               \"Power Factor\": pd.DataFrame(columns=columns, index = dataframes.keys())}\n",
    "\n",
    "# Loop through dictionary of substation dataframes\n",
    "for substation, substation_df in dataframes.items():\n",
    "    # Extract features\n",
    "    for feature in dict_Features:\n",
    "        dict_Features[feature].loc[substation] = substation_df[feature].describe().loc[['mean','max','min','std']]\n",
    "        dict_Features[feature].loc[substation]['median'] = substation_df[feature].median()\n",
    "        dict_Features[feature].loc[substation]['var'] = substation_df[feature].var()\n",
    "        dict_Features[feature].loc[substation]['sum'] = substation_df[feature].sum()\n",
    "        \n",
    "dict_Features['Active Power [kW]'].head(5)\n",
    "#dict_Features['Reactive Power [kVAr]'].head(5)\n",
    "#dict_Features['Apparent Power [kVA]'].head(5)\n",
    "#dict_Features['Power Factor'].head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Power [kW]: Classify substations as based on amount of Active Power they are generating </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "high_kW_threshold = 50 # kW\n",
    "medium_kW_threshold = 25 # kW\n",
    "\n",
    "#Initialize empty lists\n",
    "high_kW  = []\n",
    "med_kW = []\n",
    "low_kW = []\n",
    "\n",
    "#Loop through dictionary of substation dataframes to categorize substations -- this example uses 'mean'\n",
    "for key in dataframes.keys():\n",
    "    if dict_Features['Active Power [kW]'].loc[key]['mean'] > high_kW_threshold:\n",
    "        high_kW.append(key)\n",
    "    elif dict_Features['Active Power [kW]'].loc[key]['mean'] > medium_kW_threshold:\n",
    "        med_kW.append(key)\n",
    "    else:\n",
    "        low_kW.append(key)\n",
    "        \n",
    "#Check how many substations are in each list\n",
    "print('High:', len(high_kW), '\\n',\n",
    "      'Med:', len(med_kW), '\\n',\n",
    "      'Low:', len(low_kW))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reactive Power [kVAr]: Classify substations based on amount of Reactive Power they are generating </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_kVar_threshold = 1\n",
    "medium_kVar_threshold = -1\n",
    "\n",
    "high_kVAr  = []\n",
    "med_kVAr = []\n",
    "low_kVAr = []\n",
    "\n",
    "for key in dataframes.keys():\n",
    "    if dict_Features['Reactive Power [kVAr]'].loc[key]['mean'] > high_kVar_threshold:\n",
    "        high_kVAr.append(key)\n",
    "    elif dict_Features['Reactive Power [kVAr]'].loc[key]['mean'] > medium_kVar_threshold:\n",
    "        med_kVAr.append(key)\n",
    "    else:\n",
    "        low_kVAr.append(key)\n",
    "        \n",
    "print('High:', len(high_kVAr), '\\n',\n",
    "      'Med:', len(med_kVAr), '\\n',\n",
    "      'Low:', len(low_kVAr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power Factor: Ratio of Apparent power to Active power (kVA/kW)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_PF_threshold = 0.95\n",
    "medium_PF_threshold = 0.8\n",
    "\n",
    "high_PF  = []\n",
    "med_PF = []\n",
    "low_PF = []\n",
    "\n",
    "for key in dataframes.keys():\n",
    "    if dict_Features['Power Factor'].loc[key]['mean'] > high_PF_threshold:\n",
    "        high_PF.append(key)\n",
    "    elif dict_Features['Power Factor'].loc[key]['mean'] > medium_PF_threshold:\n",
    "        med_PF.append(key)\n",
    "    else:\n",
    "        low_PF.append(key)\n",
    "        \n",
    "print('High:', len(high_PF), '\\n',\n",
    "      'Med:', len(med_PF), '\\n',\n",
    "      'Low:', len(low_PF))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at substations that are high or low in both kW and kVAr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_performance = []\n",
    "low_performance = []\n",
    "for key in dataframes.keys():\n",
    "    if key in high_kW and key in high_kVAr:\n",
    "        high_performance.append(key)\n",
    "    if key in low_kW and key in low_kVAr:\n",
    "        low_performance.append(key)\n",
    "print('High performance substations', high_performance, '\\n',\n",
    "      'Low performance substations', low_performance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "Ientify leading, lagging, and unity substations: </br>\n",
    "Leading - generating more reactive power than consuming </br>\n",
    "Lagging - consuming more reactive power than generating </br>\n",
    "Unity - generating and consuming same amount of reactive power \n",
    "\n",
    "Brainstorm further summary statistics with Elnaz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore time usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a dictionary of substation dataframes, returns a dictionary of the peak hour (kW) for each substation\n",
    "'''\n",
    "def extract_peak_hours(dataframes):\n",
    "    # Create an empty dictionary to store the peak hour for each substation \n",
    "\n",
    "    \n",
    "    for substation, substation_df in dataframes.items():\n",
    "        \n",
    "        # Group the data by hour\n",
    "        hourly_data = substation_df.groupby(substation_df['Datetime'].dt.hour)['Active Power [kW]'].sum()\n",
    "\n",
    "        # Normalize the data to get probability\n",
    "        hourly_data = hourly_data / hourly_data.sum()\n",
    "        \n",
    "        # Store the peak hour for this substation in the dictionary \n",
    "        peak_hours[substation] = hourly_data[hourly_data == hourly_data.max()].index.tolist()\n",
    "    \n",
    "    return peak_hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of peak hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_hours = extract_peak_hours(dataframes)\n",
    "all_peak_hours = [item for sublist in peak_hours.values() for item in sublist]\n",
    "plt.hist(all_peak_hours)\n",
    "plt.xlabel(\"Hour of the day\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of peak hours\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extract_peak_hours_reactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average daily profile: Aggregate the load from all substations in each hour over the past month, and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "features = ['Active Power [kW]', 'Reactive Power [kVAr]', 'Apparent Power [kVA]', 'Power Factor']\n",
    "\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "all_data = pd.concat(list(dataframes.values()))\n",
    "\n",
    "# Extract the hour of the day from the datetime\n",
    "all_data['hour'] = all_data['Datetime'].dt.hour\n",
    "\n",
    "# Group the data by hour of the day\n",
    "grouped_data = all_data.groupby('hour').mean()\n",
    "\n",
    "# Plot the average daily load profile -- all features on one plot\n",
    "grouped_data[features].plot()\n",
    "plt.xlabel(\"Hour of the day\")\n",
    "plt.show()\n",
    "\n",
    "# Create function to plot the average daily load profile for a single feature\n",
    "def plot_power(dataframe, feature):\n",
    "    plt.plot(dataframe[feature])\n",
    "    plt.ylabel(feature)\n",
    "    plt.xlabel(\"Hour of the day\")\n",
    "    plt.show()\n",
    "\n",
    "#Plot features solo \n",
    "for feature in features:\n",
    "    plot_power(grouped_data, feature)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21/01/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio analysis of daily load high and low periods\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of peak/valley to average daily load (Active Power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a dictionary of substation dataframes and finds the ratio of the peak and valley hours to the average daily load (amplitudes)\n",
    "'''\n",
    "def extract_amplitudes(dataframes):\n",
    "    #Dictionaries to store the peak and valley ratios for each substation\n",
    "    peak_ratios = {}\n",
    "    valley_ratios = {}\n",
    "    \n",
    "    for substation, substation_df in dataframes.items():\n",
    "    \n",
    "        # Group the data by hour\n",
    "        hourly_data = substation_df.groupby(substation_df['Datetime'].dt.hour)['Active Power [kW]'].sum()\n",
    "\n",
    "        # Get the ratio of the peak hour to the average daily load and store in the dictionary\n",
    "        peak_ratios[substation] = hourly_data.max() / hourly_data.mean()\n",
    "        \n",
    "        # Get the ratio of the valley hour to the average daily load and store in the dictionary\n",
    "        valley_ratios[substation] = hourly_data.min() / hourly_data.mean()\n",
    "    \n",
    "    return peak_ratios, valley_ratios\n",
    "\n",
    "peak_ratios, valley_ratios = extract_amplitudes(dataframes)\n",
    "\n",
    "#Plot the distribution of peak ratios\n",
    "plt.hist(list(peak_ratios.values()))\n",
    "plt.xlabel(\"Ratio of peak hour to average daily load\")\n",
    "plt.ylabel(\"Number of substations\")\n",
    "plt.show()\n",
    "\n",
    "#Plot the distribution of valley ratios\n",
    "plt.hist(list(valley_ratios.values()))\n",
    "plt.xlabel(\"Ratio of valley hour to average daily load\")\n",
    "plt.ylabel(\"Number of substations\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long Active Power > (1.5 * average daily load) per day </br>\n",
    "How long Active Power < (0.75 * average daily load) per day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a dictionary of substation dataframes and finds:\n",
    "on average how long the active power is more than 1.5 * average daily load\n",
    "on average how long the active power is less than 0.75 * average daily load\n",
    "\n",
    "NOTE: For whatever time frame we end up looking at, we may not want to compare the average of each hour to the average daily load, but rather the actual value in each hour over the entire time frame to avareage daily load, and then divide that by the number of days in the time period. Let me know. \n",
    "'''\n",
    "def estimate_hot_or_cold(dataframes):\n",
    "    #Dictionaries to store the peak and valley ratios for each substation\n",
    "    hot_times = {}\n",
    "    cold_times = {}\n",
    "    \n",
    "    for substation, substation_df in dataframes.items():\n",
    "    \n",
    "        # Group the data by hour\n",
    "        hourly_data = substation_df.groupby(substation_df['Datetime'].dt.hour)['Active Power [kW]'].sum()\n",
    "        \n",
    "        # Get ratio of each hour to the mean\n",
    "        hourly_data = hourly_data / hourly_data.mean()\n",
    "        \n",
    "        # Get the number of hours per day that the active power is more than 1.5 * average daily load\n",
    "        hot_times[substation] = hourly_data[hourly_data > 1.5].count() \n",
    "        \n",
    "        # Get the number of hours per day that the active power is less than 0.75 * average daily load\n",
    "        cold_times[substation] = hourly_data[hourly_data < 0.75].count() \n",
    "    \n",
    "    return hot_times, cold_times\n",
    "\n",
    "hot_times, cold_times = estimate_hot_or_cold(dataframes)\n",
    "\n",
    "#Plot the distribution of how many hours per day each substation is 'running hot' on average\n",
    "plt.hist(list(hot_times.values()))\n",
    "plt.xlabel(\"Number of hours above 1.5 * average daily load\")\n",
    "plt.ylabel(\"Number of substations\")\n",
    "plt.show()\n",
    "\n",
    "#Plot the distribution of how many hours per day each substation is 'running cold' on average\n",
    "plt.hist(list(cold_times.values()))\n",
    "plt.xlabel(\"Number of hours below 0.75 * average daily load\")\n",
    "plt.ylabel(\"Number of substations\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothness of signal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root mean square (RMS) deviation: </br>\n",
    "Measure of average deviation of the signal from its mean value. \n",
    "Lower RMS = smoother, Higher RMS = more fluctuations.\n",
    "\n",
    "Variance: Low variance = more consistent, and therefore smooth and flat signal \n",
    "\n",
    "Kurtosis: Signal with high kurtosis has heavy-tailed distribution and therefore greater fluctations than signal with low kurtosis\n",
    "\n",
    "Short-Time Fourier Transform (STFT): Extract the frequency component of the signal, and the smoothness can be estimated by observing the frequency component. Using standard deviation of the fluctuation in active power, a signal with less fluctuation in frequency component is smoother. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a dictionary of substation dataframes and returns a dictionary of RMS deviation and variance for each substation\n",
    "'''\n",
    "\n",
    "def compute_smoothness_metrics(dataframes):\n",
    "    #Dictionaries to store the peak and valley ratios for each substation\n",
    "    rms = {}\n",
    "    variance = {}\n",
    "    kurtosis = {}\n",
    "    stft_smoothness = {}\n",
    "    \n",
    "    for substation, substation_df in dataframes.items():\n",
    "    \n",
    "        # Root-mean square\n",
    "        rms[substation] = np.sqrt(np.mean(substation_df['Active Power [kW]']**2))\n",
    "    \n",
    "        # Variance\n",
    "        variance[substation] = np.var(substation_df['Active Power [kW]'])\n",
    "        \n",
    "        # Kurtosis\n",
    "        kurtosis[substation] = substation_df['Active Power [kW]'].kurtosis()\n",
    "        \n",
    "        # Short Time Fourier Transform (STFT) \n",
    "        # Apply STFT to the active power signal (f = array of sample frequencies, t = array of segment times, Zxx is ndarray of STFT coefficients of Active Power signal)\n",
    "        f,t, Zxx = stft(substation_df['Active Power [kW]'].values, fs=1, window='hann', nperseg=len(substation_df['Active Power [kW]'].values))\n",
    "\n",
    "        # Calculate the average power of the STFT coefficients\n",
    "        avg_power = np.mean(np.abs(Zxx) ** 2, axis=0)\n",
    "\n",
    "        # Calculate the standard deviation of the average power\n",
    "        std_dev = np.std(avg_power)\n",
    "        \n",
    "        stft_smoothness[substation] = std_dev\n",
    "\n",
    "    return rms, variance, kurtosis, stft_smoothness\n",
    "\n",
    "rms, var, kur, stft_smoothness = compute_smoothness_metrics(dataframes)\n",
    "\n",
    "\n",
    "#Plot the distribution of RMS deviation for each substation\n",
    "plt.hist(list(rms.values()))\n",
    "plt.xlabel(\"RMS deviation of active power\")\n",
    "plt.ylabel(\"Frequency\")    \n",
    "plt.show() \n",
    "\n",
    "#Plot the distribution of Variance for each substation\n",
    "plt.hist(list(var.values()))\n",
    "plt.xlabel(\"Variance of active power\")\n",
    "plt.ylabel(\"Frequency\")    \n",
    "plt.show() \n",
    "\n",
    "#Plot the distribution of Kurtosis for each substation\n",
    "plt.hist(list(kur.values()))\n",
    "plt.xlabel(\"Kurtosis of active power\")\n",
    "plt.ylabel(\"Frequency\")    \n",
    "plt.show() \n",
    "\n",
    "#Plot the distribution of smoothness based on stft coefficient std dev for each substation\n",
    "plt.hist(list(stft_smoothness.values()))\n",
    "plt.xlabel(\"Smoothness\")\n",
    "plt.ylabel(\"Number of substations\")\n",
    "\n",
    "#Descriptive statistics of smoothness\n",
    "print(pd.Series(stft_smoothness.values()).describe().apply(lambda x: format(x, 'f')))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot of consumption per hour for each substation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a dictionary of substation dataframes and plots a boxplot with hour of the day on the x-axis and active power (kW) on the y-axis for each substation\n",
    "'''\n",
    "\n",
    "def plot_hourly_consumption(substation_data):\n",
    "    for substation, df in substation_data.items():\n",
    "        # Extract the hour of the day from the timestamp\n",
    "        df['hour'] = df['Datetime'].dt.hour\n",
    "        \n",
    "        # Create an empty list to store the data for each hour\n",
    "        hourly_data = [[] for i in range(24)]\n",
    "        # Add the active power (kW) data to the corresponding hour list\n",
    "        for i, row in df.iterrows():\n",
    "            hourly_data[row['hour']].append(row['Active Power [kW]'])\n",
    "\n",
    "        # Create a box plot of the hourly data\n",
    "        plt.boxplot(hourly_data,labels=range(24))\n",
    "        plt.title(f'Hourly Consumption for Substation {substation}')\n",
    "        plt.xlabel('Hour of the Day')\n",
    "        plt.ylabel('Active Power (kW)')\n",
    "        plt.show()\n",
    "plot_hourly_consumption(dataframes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Distribution of peak hours for each substation separately </br>\n",
    "\n",
    "For reactive power, separate substations based on positive / negative ones? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means: Clustering on Active Power and Reactive Power and Silhouette analysis to determine number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters to test \n",
    "k_range = range(2, 20)\n",
    "\n",
    "\n",
    "features_of_interest = []\n",
    "for substation, substation_df in dataframes.items():\n",
    "    # Extract features\n",
    "    features_of_interest.append([substation_df['Active Power [kW]'].mean(),\n",
    "              substation_df['Reactive Power [kVAr]'].mean()])\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in k_range:\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    kmeans.fit(features_of_interest)\n",
    "    \n",
    "    # Calculate silhouette score for this k\n",
    "    silhouette_scores.append(silhouette_score(features_of_interest, kmeans.labels_))\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.plot(k_range, silhouette_scores)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()\n",
    "\n",
    "# Assign cluster labels to substations\n",
    "substation_clusters = {}\n",
    "for i, substation in enumerate(dataframes.keys()):\n",
    "    substation_clusters[substation] = kmeans.labels_[i]\n",
    "\n",
    "# Print the cluster assignments\n",
    "print(\"Clusters based on Active and Reactive Power\")\n",
    "print(substation_clusters)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means: Clustering based on RMS, Variance, Kurtosis, and STFT Smoothness and Silhouette analysis to determine number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_means(substation_dataframes, k_range):\n",
    "    \n",
    "    features_of_interest = []\n",
    "    \n",
    "    \n",
    "    for substation, metric in substation_dataframes.items():\n",
    "        # Extract features - there is only one value per dataframe for each of the smoothness metrics, so we can just use the entire object \n",
    "        features_of_interest.append([metric])\n",
    "\n",
    "    all_substation_clusters = {}\n",
    "    silhouette_scores = []\n",
    "    for k in k_range:\n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(features_of_interest)\n",
    "        \n",
    "        # Calculate silhouette score for this k\n",
    "        silhouette_scores.append(silhouette_score(features_of_interest, kmeans.labels_))\n",
    "\n",
    "        # Assign cluster labels to substations\n",
    "        substation_clusters = {}\n",
    "        for i, substation in enumerate(substation_dataframes.keys()):\n",
    "            substation_clusters[substation] = kmeans.labels_[i]\n",
    "\n",
    "        # Save the cluster assignments \n",
    "        all_substation_clusters[k] = substation_clusters\n",
    "        \n",
    "    # Plot the silhouette scores\n",
    "    plt.plot(k_range, silhouette_scores)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return all_substation_clusters\n",
    "\n",
    "k_range = range(2,21)\n",
    "\n",
    "print(\"RMS Clustering\")\n",
    "rms_clusters = k_means(rms, k_range)\n",
    "\n",
    "print(\"Variance Clustering\")\n",
    "var_clusters =k_means(var, k_range)\n",
    "\n",
    "print(\"Kurtosis Clustering\")\n",
    "kur_clusters =k_means(kur, k_range)\n",
    "\n",
    "print(\"STFT Clustering\")\n",
    "stft_clusters =k_means(stft_smoothness, k_range)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing cluster outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This approach assumes the keys are in the same order for all dictionaries (which I think actually works here, since even though the are hashed it was run the same way and it gives the same answers every time it is run)\n",
    "\n",
    "# TODO: Save these values in a dictionary and compare between k's \n",
    "\n",
    "for k in k_range:\n",
    "    print(\"k = \", k)\n",
    "    \n",
    "    # Compare count and percentage of matching cluster assignments between RMS and Variance\n",
    "    print(\"# matching between RMS and Var:\", sum(a == b for a,b in zip(rms_clusters[k].values(), var_clusters[k].values())))\n",
    "    print(\"% matching between RMS and Var:\", round(sum(a == b for a,b in zip(rms_clusters[k].values(), var_clusters[k].values())) / len(var_clusters[k].values()), 3))\n",
    "\n",
    "    # Compare count and percentage of matching cluster assignments between all lists\n",
    "    print(\"# matching between all lists:\", sum(a == b and b == c and c == d for a,b,c,d in zip(rms_clusters[k].values(), var_clusters[k].values(), kur_clusters[k].values(), stft_clusters[k].values())))\n",
    "    print(\"# matching between all lists:\", round(sum(a == b and b == c and c == d for a,b,c,d in zip(rms_clusters[k].values(), var_clusters[k].values(), kur_clusters[k].values(), stft_clusters[k].values())) / len(var_clusters[k].values()), 3))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot substations on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get location data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "Analyze the substations that are outliers on the peak hours histogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25/01/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Identify weird substations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of peak hours for each substation separately \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active / Reactive Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peak_hour_distributions(dataframes, active):\n",
    "    for substation, substation_data in dataframes.items():\n",
    "        substation_data['Date'] = substation_data['Datetime'].dt.date\n",
    "        substation_data['Hour'] = substation_data['Datetime'].dt.hour\n",
    "        \n",
    "        # Group the data by date\n",
    "        daily_groups = substation_data.groupby(['Date'])\n",
    "\n",
    "        if active:\n",
    "        # Find the hour of peak Active Power for each date \n",
    "            peak_hour = daily_groups['Active Power [kW]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "                        # Count the number of times each hour appears as the peak hour for each day\n",
    "            peak_counts = peak_hour.value_counts()\n",
    "            \n",
    "            plt.bar(peak_counts.index, peak_counts.values)\n",
    "            plt.xlabel('Hour of the day')\n",
    "            plt.ylabel('Frequency of being peak hour')\n",
    "            plt.title(f'Peak Hour Distribution for {substation}')\n",
    "            plt.savefig(f'peak_hr_histograms/peak_hour_distribution_{substation}.png')\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "                # Find the hour of peak positive and negative Reactive power for each day\n",
    "            peak_hour_max = daily_groups['Reactive Power [kVAr]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "            peak_hour_min = daily_groups['Reactive Power [kVAr]'].idxmin().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "            \n",
    "                # Count the number of times each hour appears as the peak hour for each day\n",
    "            peak_counts_max = peak_hour_max.value_counts()\n",
    "            peak_counts_min = peak_hour_min.value_counts()\n",
    "            \n",
    "            plt.bar(peak_counts_max.index, peak_counts_max.values)\n",
    "            plt.xlabel('Hour of the day')\n",
    "            plt.ylabel('Frequency of being peak hour')\n",
    "            plt.title(f'Positive Reactive Power Peak Hour Distribution for {substation}')\n",
    "            plt.show()\n",
    "            plt.bar(peak_counts_min.index, peak_counts_min.values)\n",
    "            plt.xlabel('Hour of the day')\n",
    "            plt.ylabel('Frequency of being peak hour')\n",
    "            plt.title(f'Negative Reactive Power Peak Hour Distribution for {substation}')\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reactive Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for substation, substation_data in dataframes.items():\n",
    "    substation_data['Date'] = substation_data['Datetime'].dt.date\n",
    "    substation_data['Hour'] = substation_data['Datetime'].dt.hour\n",
    "    \n",
    "    # Group the data by date\n",
    "    daily_groups = substation_data.groupby(['Date'])\n",
    "\n",
    "    # Find the hour of peak positive and negative Reactive power for each day\n",
    "    peak_hour_max = daily_groups['Reactive Power [kVAr]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "    peak_hour_min = daily_groups['Reactive Power [kVAr]'].idxmin().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "    \n",
    "    # Count the number of times each hour appears as the peak hour for each day\n",
    "    peak_counts_max = peak_hour_max.value_counts()\n",
    "    peak_counts_min = peak_hour_min.value_counts()\n",
    "    \n",
    "    plt.bar(peak_counts_max.index, peak_counts_max.values)\n",
    "    plt.xlabel('Hour of the day')\n",
    "    plt.ylabel('Frequency of being peak hour')\n",
    "    plt.title(f'Positive Reactive Power Peak Hour Distribution for {substation}')\n",
    "    plt.show()\n",
    "    plt.bar(peak_counts_min.index, peak_counts_min.values)\n",
    "    plt.xlabel('Hour of the day')\n",
    "    plt.ylabel('Frequency of being peak hour')\n",
    "    plt.title(f'Negative Reactive Power Peak Hour Distribution for {substation}')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='orange'> Year-long Substation Data (Jan 1 2022 - Jan 1 - 2023) </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='lightgreen'> Read Data </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeder level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the folder path\n",
    "folder_path = \"../Yearly Substation Data/\"\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Extract the substation name from the file name\n",
    "        substation_name = file_name.split(\" \")[2:]\n",
    "        substation_name = substation_name[:substation_name.index(\"POWER\")]\n",
    "        substation_name = \" \".join(substation_name)\n",
    "        \n",
    "        # Read the CSV file into a pandas dataframe\n",
    "        df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "        # Convert date column to datetime -- using this as opportunity to skip over empty dataframes (if you don't want this, add \"errors = 'ignore'\" to the to_datetime command)\n",
    "        try:\n",
    "            df['Datetime'] = pd.to_datetime(df['Date (Dublin. Edinburgh. Lisbon. London)']) \n",
    "        except:\n",
    "            continue\n",
    "        # Drop old date column \n",
    "        df.drop('Date (Dublin. Edinburgh. Lisbon. London)', axis=1, inplace=True)\n",
    "        # Add the dataframe to the dictionary with the substation ID as the key\n",
    "        dataframes[substation_name] = df\n",
    "\n",
    "# Print the dataframe keys to check that the IDs processed appropriately\n",
    "print(dataframes.keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busbar level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substation 4626005100 Mill St flats POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626001300 Gloucester Green POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618007020 Hockmore Street POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626004180 osney mead pylon  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4614005070 queens lane telephone exchange POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618006160 minchery farm pumping station POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4904024005 Dan Read Parade POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4600006670 brookhampton POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4606001020 The Grove Deddington POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4907006120 Paradise Street RMU B POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4606001030 windmill st deddington POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626003020 Osney local POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626004340 southern by pass POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4911006100 woodcroft POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4640007040 Barry Avenue POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4606004360 hempton road duns tew POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4602006180 edinburgh drive pillar 2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618002040 Nowell road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626003100 henry road T2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618007100 Morris house (barns road) POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4602003070 croft avenue 1 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4911006040 kennington church (cow lane) POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626002020 Gloucester lane tx 1 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4605017210 Sherwood close  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4605010210 Springfield Road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618006200 medawar centre  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618002030 Desborough crescent POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618008040 Danvers Road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618007040 banjo road tx1  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626003090 henry road T1 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4600015150 87 Howard Street  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618003120 cornwallis road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618001220 campbell road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4602006180 edinburgh drive 1 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626006020 osney bridge street POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4602003070 Croft Avenue 2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4605017240 kingfisher way ss POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00-2.csv \n",
      "\n",
      "Substation 4626002020 Gloucester lane tx 2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618008020 devereaux place  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4610003020 Ambrosden Park Rise  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4606002080 hopcraft lane POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4911003020 Sandford lane industrial POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626006150 seacourt road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4907001020 blue boar quad RMU3 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626001360 Wellington square  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4904001020 Neolithic POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4911006020 poplar grove garages POWER F1 All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00-2.csv \n",
      "\n",
      "Substation 4600006400 garsington village hall POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626003220 the perch binsey POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618002100 Rivermead Road SS POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618001080 Hillsborough Close POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618003040 henley avenue POWER Busbar All MED AgHour FROM 2023_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4600007020 Bridge View Watlington Road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618008090 units 1314 Oxford science pk POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4011002280 high street charlgrove POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626002080 ashmolean museum  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4910011180 park town POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4904022150 Tappins Coaches POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4606001040 hempton road gm POWER Busbar All MED AgHour FROM 2022_01_19 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4605005013 chepstow drive POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618003100 Church Way Iffley POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618002060 Thames view Road no2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618002050 Thames View Road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4904020100 Drake Avenue POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618002025 rose hill community centre POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626001020 Ferry hinksey road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618001020 newman road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618005100 Little more hospital POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4913001455 Chilton Garden Centre POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4640005010 regal industrial POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618005300 76 Church Road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4610003070 quintan avenue  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618007060 barns road car park  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626002080 ashmolean museum pillar POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626001280 blackwells hythe bridge st POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618005120 littlemore pk POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618002020 Fiennes Road POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4600003070 Normandy crescent  POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626006080 texas homecare POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626001500 juxon st flats POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618005240 heyford hill Lane POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4626001480 venables close POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4606001050 Earls lane deddington POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618006140 priory Road 2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4904006200 kings lane harwell POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618005280 church road fieldside POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4605002020 bicester village service station POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618007060 barns road car park 2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618003160 george more close POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618007040 banjo road tx2 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4618006140 priory rd 1 POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4006003015 hazel grove POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "Substation 4606004340 dashwood rise POWER Busbar All MED AgHour FROM 2022_01_01 00_00_00 TO 2023_01_01 00_00_00.csv \n",
      "\n",
      "dict_keys(['Mill St flats', 'Gloucester Green', 'Hockmore Street', 'osney mead pylon ', 'queens lane telephone exchange', 'minchery farm pumping station', 'Dan Read Parade', 'brookhampton', 'The Grove Deddington', 'Paradise Street RMU B', 'windmill st deddington', 'Osney local', 'southern by pass', 'woodcroft', 'Barry Avenue', 'hempton road duns tew', 'edinburgh drive pillar 2', 'Nowell road', 'henry road T2', 'Morris house (barns road)', 'croft avenue 1', 'kennington church (cow lane)', 'Gloucester lane tx 1', 'Sherwood close ', 'Springfield Road', 'medawar centre ', 'Desborough crescent', 'Danvers Road', 'banjo road tx1 ', 'henry road T1', '87 Howard Street ', 'cornwallis road', 'campbell road', 'edinburgh drive 1', 'osney bridge street', 'Croft Avenue 2', 'kingfisher way ss', 'Gloucester lane tx 2', 'devereaux place ', 'Ambrosden Park Rise ', 'hopcraft lane', 'Sandford lane industrial', 'seacourt road', 'blue boar quad RMU3', 'Wellington square ', 'Neolithic', 'poplar grove garages', 'garsington village hall', 'the perch binsey', 'Rivermead Road SS', 'Hillsborough Close', 'henley avenue', 'Bridge View Watlington Road', 'units 1314 Oxford science pk', 'high street charlgrove', 'ashmolean museum ', 'park town', 'Tappins Coaches', 'hempton road gm', 'chepstow drive', 'Church Way Iffley', 'Thames view Road no2', 'Thames View Road', 'Drake Avenue', 'rose hill community centre', 'Ferry hinksey road', 'newman road', 'Little more hospital', 'Chilton Garden Centre', 'regal industrial', '76 Church Road', 'quintan avenue ', 'barns road car park ', 'ashmolean museum pillar', 'blackwells hythe bridge st', 'littlemore pk', 'Fiennes Road', 'Normandy crescent ', 'texas homecare', 'juxon st flats', 'heyford hill Lane', 'venables close', 'Earls lane deddington', 'priory Road 2', 'kings lane harwell', 'church road fieldside', 'bicester village service station', 'barns road car park 2', 'george more close', 'banjo road tx2', 'priory rd 1', 'hazel grove', 'dashwood rise'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the folder path\n",
    "folder_path = \"../Substation Busbar Data/\"\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        print(file_name, '\\n')\n",
    "        # Extract the substation name from the file name\n",
    "        substation_name = file_name.split(\" \")[2:]\n",
    "        substation_name = substation_name[:substation_name.index(\"POWER\")]\n",
    "        substation_name = \" \".join(substation_name)\n",
    "        \n",
    "        # Read the CSV file into a pandas dataframe\n",
    "        df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "        # Convert date column to datetime -- using this as opportunity to skip over empty dataframes (if you don't want this, add \"errors = 'ignore'\" to the to_datetime command)\n",
    "        try:\n",
    "            df['Datetime'] = pd.to_datetime(df['Date (Dublin. Edinburgh. Lisbon. London)']) \n",
    "        except:\n",
    "            continue\n",
    "        # Drop old date column \n",
    "        df.drop('Date (Dublin. Edinburgh. Lisbon. London)', axis=1, inplace=True)\n",
    "        # Add the dataframe to the dictionary with the substation ID as the key\n",
    "        dataframes[substation_name] = df\n",
    "\n",
    "# Print the dataframe keys to check that the IDs processed appropriately\n",
    "print(dataframes.keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='lightgreen'> Data cleaning: Handle Missing Values </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "######Check for dataframes with missing values\n",
    "############################################################\n",
    "empty_substations = []\n",
    "for substation, df in dataframes.items():\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        empty_substations.append(substation)\n",
    "\n",
    "if not empty_substations:\n",
    "    print(\"No missing values\")\n",
    "\n",
    "\n",
    "# ############################################################\n",
    "# ###### Drop substations with no active power data ###### ######  TODO: Get rid of this, it's covered by the below\n",
    "# ############################################################ \n",
    "# empty_substations = []\n",
    "# for substation, df in dataframes.items():\n",
    "#     if abs(df['Active Power [kW]']).sum() == 0 or abs(df['Reactive Power [kVAr]']).sum() == 0:\n",
    "#         empty_substations.append(substation)\n",
    "        \n",
    "# # Drop substations without any active power data (Heny Road T2 and Edinburgh Drive 1)\n",
    "# for substation in empty_substations:\n",
    "#     print(f\"Substation {substation} has no active power data. Dropping from dataframe.\")\n",
    "#     del dataframes[substation]\n",
    "    \n",
    "\n",
    "\n",
    "########################################################################\n",
    "### Drop substations with less than 50% available active power data ####\n",
    "########################################################################\n",
    "threshold = 0.50 # 50% threshold\n",
    "substations_below_threshold = []\n",
    "for substation, df in dataframes.items():\n",
    "    count_zero = (df['Active Power [kW]'] == 0).sum()\n",
    "    if count_zero > (len(df) * threshold):\n",
    "        substations_below_threshold.append(substation)\n",
    "        \n",
    "for substation in substations_below_threshold:\n",
    "    print(f\"Substation {substation} has less than 50% available active power data. Dropping from dataframe.\")\n",
    "    del dataframes[substation]\n",
    "\n",
    "\n",
    "# No missing values\n",
    "# Substation Thames view Road no2 has less than 50% available active power data. Dropping from dataframe.\n",
    "# Substation henry road T2 has less than 50% available active power data. Dropping from dataframe.\n",
    "# Substation the perch binsey has less than 50% available active power data. Dropping from dataframe.\n",
    "# Substation edinburgh drive 1 has less than 50% available active power data. Dropping from dataframe.\n",
    "    \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all substations, drop days with any missing data\n",
    "Missing data is defined as observations where active power is 0</BR>\n",
    "<font color = 'pink'> DON'T RUN - NOT NEEDED ANYMORE </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# This function takes in a dict of substation dataframes, and for each substation drops all days of data where 0 active power is observed. \n",
    "# The assumption is that 0 active power should never happen so this is a good way to remove days where the data is missing.\n",
    "# '''\n",
    "# def remove_days_with_missing_data(dataframes):\n",
    "#     for substation, df in dataframes.items():\n",
    "#         # Get the dates of days with 0 active power\n",
    "#         df['Date'] = df['Datetime'].dt.date\n",
    "#         zero_dates = df[df['Active Power [kW]'] == 0]['Date'].unique()\n",
    "#         # Drop those dates from the dataframe\n",
    "#         dataframes[substation] = df[~df['Date'].isin(zero_dates)]  \n",
    "        \n",
    "#     return dataframes\n",
    "        \n",
    "# dataframes = remove_days_with_missing_data(dataframes)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> Data cleaning: Detect bad values (AP > 1500 or <-50), (RP ) <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bad_power_values(df, upper_threshold = 1500, lower_threshold = -50):\n",
    "    # Find Active Power values over 1500 kW\n",
    "    bad_vals = df[(df['Active Power [kW]'] > upper_threshold) | (df['Active Power [kW]'] < lower_threshold)]\n",
    "    \n",
    "    drop_dates = bad_vals['Datetime'].dt.date.unique()\n",
    "\n",
    "    df = df[~df['Datetime'].dt.date.isin(drop_dates)]\n",
    "    \n",
    "    return df \n",
    "    \n",
    "for substation, df in dataframes.items():\n",
    "    dataframes[substation] = detect_bad_power_values(df)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='lightgreen'> Data transformation: by season and time of week </font> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_season(df):\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df['Month'] = df['Datetime'].dt.month\n",
    "    \n",
    "    spring = df[(df['Month'] >= 3) & (df['Month'] <= 5)]\n",
    "    summer = df[(df['Month'] >= 6) & (df['Month'] <= 8)]\n",
    "    fall = df[(df['Month'] >= 9) & (df['Month'] <= 11)]\n",
    "    winter = df[(df['Month'] == 12) | ((df['Month'] >= 1) & (df['Month'] <= 2))]\n",
    "    \n",
    "    return spring, summer, fall, winter\n",
    "\n",
    "substation_dataframes = {}\n",
    "for substation, df in dataframes.items():\n",
    "    spring, summer, fall, winter = split_by_season(df)\n",
    "    substation_dataframes[substation] = {'spring': spring, 'summer': summer, 'fall': fall, 'winter': winter}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the season dataframes by time of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split seasonal dataframes into week and week-end dataframes\n",
    "def split_weekend_week(df):\n",
    "    df['weekday'] = df['Datetime'].dt.weekday\n",
    "    week_df = df[df['weekday'].isin([0,1,2,3,4])]\n",
    "    weekend_df = df[df['weekday'].isin([5,6])]\n",
    "    return week_df, weekend_df\n",
    "\n",
    "chopped_substation_dfs = {}\n",
    "for substation, season_dict in substation_dataframes.items():\n",
    "    chopped_substation_dfs[substation] = {}\n",
    "    for season, df in season_dict.items():\n",
    "        week_df, weekend_df = split_weekend_week(df)\n",
    "        chopped_substation_dfs[substation][season] = {}\n",
    "        chopped_substation_dfs[substation][season]['week'] = week_df\n",
    "        chopped_substation_dfs[substation][season]['weekend'] = weekend_df\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='lightgreen'> Data cleaning: Drop the substations with not enough data in given season/time-of-week subset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop substations with not enough data in sub-set\n",
    "drop_list = []\n",
    "for substation, seasons in chopped_substation_dfs.items():\n",
    "    for season, days in seasons.items():\n",
    "        for time_of_week, df in days.items():\n",
    "            days_with_data = len(df['Datetime'].dt.date.unique())\n",
    "            if time_of_week == 'week':\n",
    "                if days_with_data < 33: # 33 days is ~1/2 of the 65 days in a season during the week \n",
    "                    print(f\"Substation {substation} has only {days_with_data} days of data in {season} {time_of_week} (<~1/2 of what should be there). Dropping from analysis.\")\n",
    "                    drop_list.append(str(substation) + \"_\" + str(season) + \"_\" + str(time_of_week))\n",
    "            else:\n",
    "                if days_with_data < 13: #(has less than half of weekends)\n",
    "                    print(f'Substation {substation} has only {days_with_data} days of data in {season} {time_of_week} (<~1/2 of what should be there). Dropping from analysis.')\n",
    "                    drop_list.append(str(substation) + \"_\" + str(season) + \"_\" + str(time_of_week))\n",
    "           # print(substation, season, time_of_week, len(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='lightgreen'> Drop the underfull substation/season/time of week combinations </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of sub-datasets to drop,\", len(drop_list))\n",
    "for to_drop in drop_list:\n",
    "    substation = to_drop.split(\"_\")[0]\n",
    "    season = to_drop.split(\"_\")[1]\n",
    "    time_of_week = to_drop.split(\"_\")[2]\n",
    "    del chopped_substation_dfs[substation][season][time_of_week]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='lightgreen'> Data cleaning: Detect Outliers </font> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peak_hour_distributions(dataframes, active):\n",
    "    for substation, substation_data in dataframes.items():\n",
    "        substation_data['Date'] = substation_data['Datetime'].dt.date\n",
    "        substation_data['Hour'] = substation_data['Datetime'].dt.hour\n",
    "        \n",
    "        # Group the data by date\n",
    "        daily_groups = substation_data.groupby(['Date'])\n",
    "\n",
    "        if active:\n",
    "        # Find the hour of peak Active Power for each date \n",
    "            peak_hour = daily_groups['Active Power [kW]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "                        # Count the number of times each hour appears as the peak hour for each day\n",
    "            peak_counts = peak_hour.value_counts()\n",
    "            \n",
    "            plt.bar(peak_counts.index, peak_counts.values)\n",
    "            plt.xlabel('Hour of the day')\n",
    "            plt.ylabel('Frequency of being peak hour')\n",
    "            plt.title(f'Peak Hour Distribution for {substation}')\n",
    "            plt.savefig(f'peak_hr_histograms/peak_hour_distribution_{substation}.png')\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "                # Find the hour of peak positive and negative Reactive power for each day\n",
    "            peak_hour_max = daily_groups['Reactive Power [kVAr]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "            peak_hour_min = daily_groups['Reactive Power [kVAr]'].idxmin().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "            \n",
    "                # Count the number of times each hour appears as the peak hour for each day\n",
    "            peak_counts_max = peak_hour_max.value_counts()\n",
    "            peak_counts_min = peak_hour_min.value_counts()\n",
    "            \n",
    "            plt.bar(peak_counts_max.index, peak_counts_max.values)\n",
    "            plt.xlabel('Hour of the day')\n",
    "            plt.ylabel('Frequency of being peak hour')\n",
    "            plt.title(f'Positive Reactive Power Peak Hour Distribution for {substation}')\n",
    "            plt.show()\n",
    "            plt.bar(peak_counts_min.index, peak_counts_min.values)\n",
    "            plt.xlabel('Hour of the day')\n",
    "            plt.ylabel('Frequency of being peak hour')\n",
    "            plt.title(f'Negative Reactive Power Peak Hour Distribution for {substation}')\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_peak_hour_distributions(dataframes, active = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'orange'> Data Imputation?? \n",
    "\n",
    "Compute median daily load curve for each day of the week for each substation in the dataset, and fill missing days with corresponding curve?\n",
    "Or leave missing days out </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'orange'> Feature Extraction <font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='lightgreen'> Extract features for clustering (functions) </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global features  </br>\n",
    "\n",
    "Mean value of daily load pattern </br>\n",
    "SD of daily load pattern </br>\n",
    "Max power consumption during a day  </br>\n",
    "Min power consumption during a day  </br>\n",
    "Range power consumption during a day (max - min) </br>\n",
    "Perc values above mean val  </br>\n",
    "Sum of net loads during business hours (9am-6pm) </br>\n",
    "Sum of net loads during non-business hours </br>\n",
    "Skewness of the distribution of a daily load pattern  </br>\n",
    "Kurtosis of distribution of a daily load pattern  </br>\n",
    "Mode of 5-bin histogram for daily load pattern   </br>\n",
    "Longest sub-sequence where consecutive value above mean value  </br>\n",
    "Longest period of successive increase  </br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.<font color ='pink'> Get as statistical features for the distribution of total daily loads across the year  _DEPRECATED_ <font> </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function (v1) extracts the global features for a distribution of total daily loads within the measured dataset for each substation\n",
    "'''\n",
    "\n",
    "def extract_global_features(df):\n",
    "    # convert datetime column to datetime type\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    \n",
    "    df['Date'] = df['Datetime'].dt.date\n",
    "    df['Hour'] = df['Datetime'].dt.hour\n",
    "    \n",
    "    # set datetime column as index\n",
    "    df = df.set_index('Datetime')\n",
    "    \n",
    "    # group data by day to get daily load pattern (total energy consumed in a day)\n",
    "    daily_loads = df.resample('D').sum()\n",
    "    \n",
    "    # Mean daily power consumption \n",
    "    mean_daily_load_pattern = daily_loads['Active Power [kW]'].mean()\n",
    "    \n",
    "    # Std deviation of daily power consumption\n",
    "    sd_daily_load_pattern = daily_loads['Active Power [kW]'].std()\n",
    "    \n",
    "    # Max power consumption during a day\n",
    "    max_power_consumption = daily_loads['Active Power [kW]'].max()\n",
    "    \n",
    "    # Min power consumption during a day\n",
    "    min_power_consumption = daily_loads['Active Power [kW]'].min()\n",
    "    \n",
    "    # Range of power consumption during a day\n",
    "    range_power_consumption = max_power_consumption - min_power_consumption\n",
    "\n",
    "    # Maximum within-day power consumption range \n",
    "    (daily_groups['Active Power [kW]'].max() - daily_groups['Active Power [kW]'].min()).max()\n",
    "    \n",
    "    # Percent values above mean value\n",
    "    percent_values_above_mean = (daily_loads > mean_daily_load_pattern).mean() * 100\n",
    "    \n",
    "    # Sum of net loads during business hours (9am-6pm)\n",
    "    business_hours = (df.index.hour >= 9) & (df.index.hour < 18)\n",
    "    sum_net_loads_business_hours = df[business_hours].sum()\n",
    "    \n",
    "    # Sum of net loads during non-business hours\n",
    "    non_business_hours = ~business_hours\n",
    "    sum_net_loads_non_business_hours = df[non_business_hours].sum()\n",
    "    \n",
    "    # kewness of the distribution of a daily load pattern\n",
    "    skewness_daily_load_pattern = daily_loads.skew()\n",
    "    \n",
    "    # kurtosis of distribution of a daily load pattern\n",
    "    kurtosis_daily_load_pattern = daily_loads.kurtosis()\n",
    "    \n",
    "    # mode of 5-bin histogram for daily load pattern\n",
    "    hist, bin_edges = np.histogram(daily_loads, bins=5)\n",
    "    mode_5_bin_histogram = bin_edges[np.argmax(hist)]\n",
    "    \n",
    "    # longest sub-sequence where consecutive value above mean value\n",
    "    # above_mean = daily_load_pattern > mean_daily_load_pattern\n",
    "    # longest_subseq_above_mean = above_mean.astype(int).diff().ne(0).cumsum()\n",
    "    # group = longest_subseq_above_mean.groupby(longest_subseq_above_mean).cumcount()\n",
    "    # longest_subseq_above_mean = (group + 1).max()\n",
    "    \n",
    "    # longest sub-sequence where consecutive value above mean value\n",
    "   # longest_subseq_above_mean = ((daily_load_pattern > mean_daily_load_pattern).astype(int)\n",
    "                       #           .groupby((~daily_load_pattern.astype(int)).cumsum()).sum().max())\n",
    "\n",
    "    # longest period of successive increases\n",
    "    # increase = (daily_load_pattern.diff() > 0).astype(int)\n",
    "    # longest_period_successive_increases = increase.astype(int).diff().ne(0).cumsum()\n",
    "    # group = longest_period_successive_increases.groupby(longest_period_successive_increases).cumcount()\n",
    "    # longest_period_successive_increases = (group + 1).max()\n",
    "    \n",
    "    features = {\n",
    "        'Mean value of daily load pattern (kW)': mean_daily_load_pattern,\n",
    "        'SD of daily load pattern (kW)': sd_daily_load_pattern,\n",
    "        'Max power consumption during a day (kW)': max_power_consumption,\n",
    "        'Min power consumption during a day (kW)': min_power_consumption,\n",
    "        'Range of power consumption during a day (max - min) (kW)': range_power_consumption,\n",
    "        'Percent values above mean val (%)': percent_values_above_mean,\n",
    "        'Sum of net loads during business hours (9am-6pm)': sum_net_loads_business_hours,\n",
    "        'Sum of net loads during non-business hours': sum_net_loads_non_business_hours,\n",
    "        'Skewness of the distribution of a daily load pattern': skewness_daily_load_pattern,\n",
    "        'Kurtosis of distribution of a daily load pattern': kurtosis_daily_load_pattern,\n",
    "        'Mode of 5-bin histogram for daily load pattern': mode_5_bin_histogram\n",
    "        #'Longest sub-sequence where consecutive value above mean value': longest_subseq_above_mean,\n",
    "       # 'Longest period of successive increases': longest_period_successive_increases\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(features, index=[0])\n",
    "\n",
    "# extract features for all dataframes\n",
    "all_features = []\n",
    "for substation, df in dataframes.items():\n",
    "    features = extract_global_features(df)\n",
    "    features['Substation'] = substation\n",
    "    all_features.append(features)\n",
    "    \n",
    "# concatenate all features into one dataframe\n",
    "all_features_concat = pd.concat(all_features)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'> _DEPRECATED function_\n",
    "\n",
    "2. Get as average of the values of these statistical features for each daily load in the year (i.e. for statistical feature i in I SUM(i,d=1...365)SF_n,i/365))  [so here the distribution is the distribution of values that each feature takes across the whole year].  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function (v2) gets a distribution statistical features for each daily load in dataset, and returns them in a dictionary where each key is a feature and each value is a series containing that feature's distribution for each day in the dataset.\n",
    "'''\n",
    "\n",
    "def extract_global_features_v2(substation, df):\n",
    "    # convert datetime column to datetime type\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    \n",
    "    df['Date'] = df['Datetime'].dt.date\n",
    "    df['Hour'] = df['Datetime'].dt.hour\n",
    "    \n",
    "    # set datetime column as index\n",
    "    df = df.set_index('Datetime')\n",
    "    \n",
    "    # group data by day to get daily load pattern (total energy consumed in a day)\n",
    "   # daily_loads = df.resample('D').sum()\n",
    "    \n",
    "    # # group data by day to extract any staistical features across all days\n",
    "    daily_groups = df.groupby(df['Date'])\n",
    "\n",
    "    # # Peak power consumption\n",
    "    # peak_power_conumption = df['Active Power [kW]'].max()\n",
    "    \n",
    "    # Mean daily power consumption \n",
    "    mean_powers = daily_groups['Active Power [kW]'].mean()\n",
    "    \n",
    "    # Std deviation of daily power consumption\n",
    "    sd_powers = daily_groups['Active Power [kW]'].std()\n",
    "    \n",
    "    # Max power consumption during a day\n",
    "    max_powers = daily_groups['Active Power [kW]'].max()\n",
    "    \n",
    "    # Min power consumption during a day\n",
    "    min_powers = daily_groups['Active Power [kW]'].min()\n",
    "    \n",
    "    # Range of power consumption during a day\n",
    "    range_powers = max_powers - min_powers\n",
    "    \n",
    "    # Percent values above mean value in each day\n",
    "    above_mean_counts = [(df[(df['Date'] == date) & (df['Active Power [kW]'] > mean)]).shape[0] for date, mean in mean_powers.iteritems()]\n",
    "    percentage_above_mean = [above_mean_count / 24 * 100 for i, above_mean_count in enumerate(above_mean_counts)]\n",
    "\n",
    "    # Filter the dataframe to include only the hours between 9 and 17\n",
    "    filtered_business_hours = df[(df['Hour'] >= 9) & (df['Hour'] <= 18)]\n",
    "    # Group by date\n",
    "    grouped_business_hours = filtered_business_hours.groupby(by=['Date'])\n",
    "    # Sum of net loads during business hours (9am-6pm)\n",
    "    business_hour_loads = grouped_business_hours['Active Power [kW]'].sum()\n",
    "    \n",
    "    ####################### Now for non-business hours (the opposite) #############################\n",
    "    filtered_non_business_hours = df[(df['Hour'] < 9) | (df['Hour'] > 18)]\n",
    "    grouped_non_business_hours = filtered_non_business_hours.groupby(by=['Date'])\n",
    "    non_business_hour_loads = grouped_non_business_hours['Active Power [kW]'].sum()\n",
    "    \n",
    "    # Skewness \n",
    "    skewness_daily_load_pattern = daily_groups['Active Power [kW]'].sum().skew()\n",
    "    \n",
    "    # Kurtosis \n",
    "    kurtosis_daily_load_pattern = daily_groups['Active Power [kW]'].sum().kurtosis()\n",
    "    \n",
    "    # Mode of 5-bin histogram for daily load pattern\n",
    "    hist, bin_edges = np.histogram(daily_groups['Active Power [kW]'].sum(), bins=5)\n",
    "    mode_5_bin_histogram = bin_edges[np.argmax(hist)]\n",
    "    \n",
    "    \n",
    "    features = {\n",
    "        'Mean value of daily load pattern (kW)': mean_powers.mean(),\n",
    "        'SD of daily load pattern (kW)': sd_powers.mean(),\n",
    "        'Max power consumption during a day (kW)': max_powers.max(),\n",
    "        'Min power consumption during a day (kW)': min_powers.min(),\n",
    "        'Range of power consumption during a day (max - min) (kW)': range_powers.max(),\n",
    "        'Percent values above mean val (%)': statistics.mean(above_mean_counts),\n",
    "        'Sum of net loads during business hours (9am-6pm)': business_hour_loads.mean(),\n",
    "        'Sum of net loads during non-business hours': non_business_hour_loads.mean(),\n",
    "        'Skewness of the distribution of a daily load pattern': skewness_daily_load_pattern.mean(),\n",
    "        'Kurtosis of distribution of a daily load pattern': kurtosis_daily_load_pattern.mean(),\n",
    "        'Mode of 5-bin histogram for daily load pattern': mode_5_bin_histogram\n",
    "        #'Longest sub-sequence where consecutive value above mean value': longest_subseq_above_mean,\n",
    "       # 'Longest period of successive increases': longest_period_successive_increases\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return features\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='lightgreen'> FUNCTION <br> Modify extract features function to extract Reactive Power Features as well (*wait for Elnaz guidance*) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same as v2, but option to get reactive power features as well as active power features\n",
    "'''\n",
    "\n",
    "def extract_global_features_v3(substation, df, active = True):\n",
    "    # convert datetime column to datetime type\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    \n",
    "    df['Date'] = df['Datetime'].dt.date\n",
    "    df['Hour'] = df['Datetime'].dt.hour\n",
    "    \n",
    "    # set datetime column as index\n",
    "    df = df.set_index('Datetime')\n",
    "    \n",
    "    first = True\n",
    "    for feature_of_interest in ['Active Power [kW]', 'Reactive Power [kVAr]']:\n",
    "        \n",
    "    # # Set feature of interest to either active or reactive power\n",
    "    # if active:\n",
    "    #     feature_of_interest = 'Active Power [kW]'\n",
    "    #     # group data by day to extract any staistical features across all days\n",
    "        \n",
    "    # else:\n",
    "    #     feature_of_interest = 'Reactive Power [kVAr]'\n",
    "    #     # if reactive power, group data by positive and negative and then by day \n",
    "\n",
    "        ##################################################################\n",
    "        ##################################################################\n",
    "        \n",
    "        # group data by day to get daily load pattern (total energy consumed in a day)\n",
    "        daily_groups = df.groupby(df['Date'])\n",
    "        \n",
    "        # Mean daily power consumption \n",
    "        mean_powers = daily_groups[feature_of_interest].mean()\n",
    "        \n",
    "        # Std deviation of daily power consumption\n",
    "        sd_powers = daily_groups[feature_of_interest].std()\n",
    "        \n",
    "        # Max power consumption during a day\n",
    "        max_powers = daily_groups[feature_of_interest].max()\n",
    "        \n",
    "        # Min power consumption during a day\n",
    "        min_powers = daily_groups[feature_of_interest].min()\n",
    "        \n",
    "        # Range of power consumption during a day\n",
    "        range_powers = max_powers - min_powers\n",
    "        \n",
    "        # Percent values above mean value in each day\n",
    "        above_mean_counts = [(df[(df['Date'] == date) & (df[feature_of_interest] > mean)]).shape[0] for date, mean in mean_powers.iteritems()]\n",
    "        percentage_above_mean = [above_mean_count / 24 * 100 for i, above_mean_count in enumerate(above_mean_counts)]\n",
    "\n",
    "        # Filter the dataframe to include only the hours between 9 and 17\n",
    "        filtered_business_hours = df[(df['Hour'] >= 9) & (df['Hour'] <= 18)]\n",
    "        # Group by date\n",
    "        grouped_business_hours = filtered_business_hours.groupby(by=['Date'])\n",
    "        # Sum of net loads during business hours (9am-6pm)\n",
    "        business_hour_loads = grouped_business_hours[feature_of_interest].sum()\n",
    "        \n",
    "        ####################### Now for non-business hours (the opposite) #############################\n",
    "        filtered_non_business_hours = df[(df['Hour'] < 9) | (df['Hour'] > 18)]\n",
    "        grouped_non_business_hours = filtered_non_business_hours.groupby(by=['Date'])\n",
    "        non_business_hour_loads = grouped_non_business_hours[feature_of_interest].sum()\n",
    "        \n",
    "        # Skewness \n",
    "        skewness_daily_load_pattern = daily_groups[feature_of_interest].sum().skew()\n",
    "        \n",
    "        # Kurtosis \n",
    "        kurtosis_daily_load_pattern = daily_groups[feature_of_interest].sum().kurtosis()\n",
    "        \n",
    "        # Mode of 5-bin histogram for daily load pattern\n",
    "        hist, bin_edges = np.histogram(daily_groups[feature_of_interest].sum(), bins=5)\n",
    "        mode_5_bin_histogram = bin_edges[np.argmax(hist)]\n",
    "        \n",
    "        if first:\n",
    "            features = {\n",
    "                f'Mean value of daily load pattern (kW) {feature_of_interest}' : mean_powers.mean(),\n",
    "                f'SD of daily load pattern (kW) {feature_of_interest}': sd_powers.mean(),\n",
    "                f'Max power consumption during a day (kW) {feature_of_interest}': max_powers.max(),\n",
    "                f'Min power consumption during a day (kW) {feature_of_interest}': min_powers.min(),\n",
    "                f'Range of power consumption during a day (max - min) (kW) {feature_of_interest}': range_powers.max(),\n",
    "                f'Percent values above mean val (%) {feature_of_interest}': statistics.mean(above_mean_counts),\n",
    "                f'Sum of net loads during business hours (9am-6pm) {feature_of_interest}': business_hour_loads.mean(),\n",
    "                f'Sum of net loads during non-business hours {feature_of_interest}': non_business_hour_loads.mean(),\n",
    "                f'Skewness of the distribution of a daily load pattern {feature_of_interest}': skewness_daily_load_pattern.mean(),\n",
    "                f'Kurtosis of distribution of a daily load pattern {feature_of_interest}': kurtosis_daily_load_pattern.mean(),\n",
    "                f'Mode of 5-bin histogram for daily load pattern {feature_of_interest}': mode_5_bin_histogram\n",
    "                #'Longest sub-sequence where consecutive value above mean value': longest_subseq_above_mean,\n",
    "            # 'Longest period of successive increases': longest_period_successive_increases\n",
    "            }\n",
    "        else:\n",
    "            next = {\n",
    "                f'Mean value of daily load pattern (kW) {feature_of_interest}' : mean_powers.mean(),\n",
    "                f'SD of daily load pattern (kW) {feature_of_interest}': sd_powers.mean(),\n",
    "                f'Max power consumption during a day (kW) {feature_of_interest}': max_powers.max(),\n",
    "                f'Min power consumption during a day (kW) {feature_of_interest}': min_powers.min(),\n",
    "                f'Range of power consumption during a day (max - min) (kW) {feature_of_interest}': range_powers.max(),\n",
    "                f'Percent values above mean val (%) {feature_of_interest}': statistics.mean(above_mean_counts),\n",
    "                f'Sum of net loads during business hours (9am-6pm) {feature_of_interest}': business_hour_loads.mean(),\n",
    "                f'Sum of net loads during non-business hours {feature_of_interest}': non_business_hour_loads.mean(),\n",
    "                f'Skewness of the distribution of a daily load pattern {feature_of_interest}': skewness_daily_load_pattern.mean(),\n",
    "                f'Kurtosis of distribution of a daily load pattern {feature_of_interest}': kurtosis_daily_load_pattern.mean(),\n",
    "                f'Mode of 5-bin histogram for daily load pattern {feature_of_interest}': mode_5_bin_histogram\n",
    "                #'Longest sub-sequence where consecutive value above mean value': longest_subseq_above_mean,\n",
    "            # 'Longest period of successive increases': longest_period_successive_increases\n",
    "            }\n",
    "            features.update(next)\n",
    "        first = False \n",
    "    \n",
    "    \n",
    "    return features\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='lightgreen'> FUNCTION <br>\n",
    "Get part of day in which peak hour occurs <font> <br>\n",
    "<font color = 'lightgreen'> Modify function to get Reactive Power features </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_hour_distribution_active(substation_data, time_intervals, time_labels):\n",
    "    \n",
    "    #peak_hour_part_of_day = {}\n",
    "  #  for substation, substation_data in dataframes.items():\n",
    "    # Extract Date and Hour columns from Datetime column \n",
    "    substation_data['Date'] = substation_data['Datetime'].dt.date\n",
    "    substation_data['Hour'] = substation_data['Datetime'].dt.hour\n",
    "    \n",
    "    # Group the data by Date\n",
    "    daily_groups = substation_data.groupby(['Date'])\n",
    "    \n",
    "\n",
    "   \n",
    "    # Find the hour of peak Active Power for each date \n",
    "    peak_hour = daily_groups['Active Power [kW]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "    \n",
    "    # Count the number of times each hour appears as the peak hour for each day\n",
    "    peak_counts = peak_hour.value_counts()\n",
    "    \n",
    "    # Add the distribution to the dictionary\n",
    "    # distributions[substation] = peak_counts\n",
    "    most_common_peak = peak_counts.idxmax()\n",
    "    for i, (start,end) in enumerate(time_intervals):\n",
    "      if start <= most_common_peak < end:\n",
    "        peak_part_of_day = time_labels[i]\n",
    "        \n",
    "    return peak_part_of_day\n",
    "            \n",
    "        \n",
    "def get_peak_hour_distribution_reactive(substation_data, time_intervals, time_labels):\n",
    "  \n",
    "     # peak_hour_part_of_day = {}\n",
    "    #  for substation, substation_data in dataframes.items():\n",
    "      # Extract Date and Hour columns from Datetime column \n",
    "      substation_data['Date'] = substation_data['Datetime'].dt.date\n",
    "      substation_data['Hour'] = substation_data['Datetime'].dt.hour\n",
    "      \n",
    "    # Group the data by Date\n",
    "      daily_groups = substation_data.groupby(['Date'])\n",
    "    \n",
    "      # Find the hour of Peak positive and Valley negative Reactive power for each day\n",
    "      peak_hour_max = daily_groups['Reactive Power [kVAr]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "      peak_hour_min = daily_groups['Reactive Power [kVAr]'].idxmin().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "      \n",
    "      # Count the number of times each hour appears as the peak hour for each day\n",
    "      peak_counts_max = peak_hour_max.value_counts()\n",
    "      peak_counts_min = peak_hour_min.value_counts()\n",
    "      \n",
    "      # Add the distributions to the dictionary\n",
    "      most_common_peak_max = peak_counts_max.idxmax()\n",
    "      most_common_peak_min = peak_counts_min.idxmax()\n",
    "      \n",
    "      for i, (start,end) in enumerate(time_intervals):\n",
    "        if start <= most_common_peak_max < end:\n",
    "          positive_peak_part_of_day = time_labels[i]\n",
    "        if start <= most_common_peak_min < end:\n",
    "          negative_peak_part_of_day = time_labels[i]\n",
    "      \n",
    "\n",
    "      return positive_peak_part_of_day, negative_peak_part_of_day\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='lightgreen'> Extracting features for clustering (implementation) <font>\n",
    "<font color ='lightgreen'> Extracting global features <font> <br>\n",
    "<font color ='lightgreen'> Extracting peak hour features and incorporating it into 'features' dict <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to iterate through 'chopped_substation_dfs' to get the global features and peak hour distributions for each subsattion/season/time of week combination\n",
    "# Then need to save the global features and peak hour distributions for each subsattion/season/time of week combination into a dataframe\n",
    "\n",
    "# Possibly... needs to be saved in a dictionary of dataframes\n",
    "# Define the time period labels\n",
    "time_labels = [1, 2, 3, 4, 5, 1]\n",
    "#time_labels = ['night', 'morning', 'midday', 'afternoon', 'evening', 'night']\n",
    "\n",
    "# Define the time period intervals\n",
    "time_intervals = [(0, 5), (5, 11), (11, 14), (14, 17), (17, 22), (22, 24)]\n",
    "\n",
    "features_dataframes = {}\n",
    "active = False \n",
    "for substation, season in chopped_substation_dfs.items():\n",
    "    features_dataframes[substation]  = {}\n",
    "    for season_name, times_of_week in season.items():\n",
    "        features_dataframes[substation][season_name] = {}\n",
    "        for time_of_week, df in times_of_week.items():\n",
    "            \n",
    "            # Get global features in a dictionary\n",
    "            features = extract_global_features_v3(substation, df, active)\n",
    "            \n",
    "            # Get peak hour distributions. What I really need is just for the peak hour distributions to be added to the features dictionary... hmmm... so I want a key that is 'peak hour distribution' and the value to be that distribution? but then that's different than all the other things\n",
    "            \n",
    "            for active in [True, False]:\n",
    "                if active:\n",
    "                    features['Most common Active Power peak time of day'] = get_peak_hour_distribution_active(df, time_intervals, time_labels)\n",
    "                else:\n",
    "                    features['Most common positive Reactive Power peak time of day'], features['Most common negative Reactive Power peak time of day'] = get_peak_hour_distribution_reactive(df, time_intervals, time_labels)                    \n",
    "            \n",
    "            features_dataframes[substation][season_name][time_of_week] = features \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_of_interest = 'Active Power [kW]'\n",
    "\n",
    "global_active_features = [\n",
    "        f'Mean value of daily load pattern (kW) {feature_of_interest}',\n",
    "        f'SD of daily load pattern (kW) {feature_of_interest}',\n",
    "        f'Max power consumption during a day (kW) {feature_of_interest}',\n",
    "        f'Min power consumption during a day (kW) {feature_of_interest}',\n",
    "        f'Range of power consumption during a day (max - min) (kW) {feature_of_interest}',\n",
    "        f'Percent values above mean val (%) {feature_of_interest}',\n",
    "        f'Sum of net loads during business hours (9am-6pm) {feature_of_interest}',\n",
    "        f'Sum of net loads during non-business hours {feature_of_interest}',\n",
    "        f'Skewness of the distribution of a daily load pattern {feature_of_interest}',\n",
    "        f'Kurtosis of distribution of a daily load pattern {feature_of_interest}',\n",
    "        f'Mode of 5-bin histogram for daily load pattern {feature_of_interest}',\n",
    "      \n",
    "    ]\n",
    "feature_of_interest = 'Reactive Power [kVAr]'\n",
    "\n",
    "global_reactive_features = [\n",
    "        f'Mean value of daily load pattern (kW) {feature_of_interest}',\n",
    "        f'SD of daily load pattern (kW) {feature_of_interest}',\n",
    "        f'Max power consumption during a day (kW) {feature_of_interest}',\n",
    "        f'Min power consumption during a day (kW) {feature_of_interest}',\n",
    "        f'Range of power consumption during a day (max - min) (kW) {feature_of_interest}',\n",
    "        f'Percent values above mean val (%) {feature_of_interest}',\n",
    "        f'Sum of net loads during business hours (9am-6pm) {feature_of_interest}',\n",
    "        f'Sum of net loads during non-business hours {feature_of_interest}',\n",
    "        f'Skewness of the distribution of a daily load pattern {feature_of_interest}',\n",
    "        f'Kurtosis of distribution of a daily load pattern {feature_of_interest}',\n",
    "        f'Mode of 5-bin histogram for daily load pattern {feature_of_interest}',\n",
    "    ]\n",
    "\n",
    "peak_features = ['Most common Active Power peak time of day',\n",
    "                'Most common positive Reactive Power peak time of day',\n",
    "                'Most common negative Reactive Power peak time of day']\n",
    "\n",
    "active_and_reactive_features = global_active_features + global_reactive_features + peak_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='lightgreen'> Create separate season/time-of-week dataframes for clustering <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to hold the dataframes\n",
    "df_dict = {}\n",
    "\n",
    "feature_set = ['substation'] + active_and_reactive_features\n",
    "\n",
    "# Loop through the nested dictionary\n",
    "for substation, season_dict in features_dataframes.items():\n",
    "    for season, time_dict in season_dict.items():\n",
    "        for time_of_week, feature_dict in time_dict.items():\n",
    "            # If the dataframe doesn't exist for this season/time_of_week combination, create it\n",
    "            if (time_of_week, season) not in df_dict:\n",
    "                df_dict[(time_of_week, season)] = pd.DataFrame(columns= feature_set)\n",
    "            # Create a new row of feature data for the substation\n",
    "            feature_data = {\"substation\": substation}\n",
    "            feature_data.update(feature_dict)\n",
    "\n",
    "            # Convert the feature data into a dataframe and append it to the corresponding dataframe in df_dict\n",
    "            df_dict[(time_of_week, season)] = df_dict[(time_of_week, season)].append(feature_data, ignore_index=True)\n",
    "            \n",
    "            \n",
    "        \n",
    "# Access the dataframes using a tuple of (time_of_week, season) as the key\n",
    "spring_weekday_df = df_dict[(\"week\", \"spring\")]\n",
    "spring_weekend_df = df_dict[(\"weekend\", \"spring\")]\n",
    "summer_weekday_df = df_dict[(\"week\", \"summer\")]\n",
    "summer_weekend_df = df_dict[(\"weekend\", \"summer\")]\n",
    "fall_weekday_df = df_dict[(\"week\", \"fall\")]\n",
    "fall_weekend_df = df_dict[(\"weekend\", \"fall\")]\n",
    "winter_weekday_df = df_dict[(\"week\", \"winter\")]\n",
    "winter_weekend_df = df_dict[(\"weekend\", \"winter\")]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'lightgreen'> Feature Correlation Matrix <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "for k,v in df_dict:\n",
    "    df = df_dict[k,v]\n",
    "\n",
    "    #df_dict[k,v]['Most common Active Power peak time of day'] = df_dict[k,v]['Most common Active Power peak time of day'].astype(float)\n",
    "    df = df.astype({'Most common Active Power peak time of day':'float',\n",
    "                    'Most common positive Reactive Power peak time of day':'float',\n",
    "                    'Most common negative Reactive Power peak time of day': 'float'})\n",
    "    \n",
    "     # Build a correlatrion matrix between all the features in the dataframe (excluding cluster labels)\n",
    "    corr_matrix = df.loc[:, ~df.columns.str.contains('cluster')].corr()\n",
    "    \n",
    "    # Build active and reactive features only correlation matrices\n",
    "    ap_features_corr_matrix = df.loc[:, global_active_features + ['Most common Active Power peak time of day']].corr()\n",
    "    rp_features_corr_matrix = df.loc[:, global_reactive_features + ['Most common positive Reactive Power peak time of day', 'Most common negative Reactive Power peak time of day' ]].corr()\n",
    "\n",
    "    # Create labels for the plots\n",
    "    feature = 'Active'\n",
    "    active_feature_labels = [f'GF-1 ({feature})', f'GF-2 ({feature})', f'GF-3 ({feature})', f'GF-4 ({feature})', f'GF-5 ({feature})',\n",
    "                             f'GF-6 ({feature})', f'GF-7 ({feature})', f'GF-8 ({feature})', f'GF-9 ({feature})', f'GF-10 ({feature})', f'GF-11 ({feature})', f'PF-1 ({feature})']\n",
    "    feature = 'Reactive'\n",
    "    reactive_feature_labels =[f'GF-1 ({feature})', f'GF-2 ({feature})', f'GF-3 ({feature})', f'GF-4 ({feature})', f'GF-5 ({feature})',\n",
    "                             f'GF-6 ({feature})', f'GF-7 ({feature})', f'GF-8 ({feature})', f'GF-9 ({feature})', f'GF-10 ({feature})', f'GF-11 ({feature})', f'PF-1 ({feature})', f'PF-2 ({feature})']\n",
    "    \n",
    "    # All feature labels (reorganized so in correct order)\n",
    "    all_feature_labels = active_feature_labels[:-1] + reactive_feature_labels[:-2] + ['PF-1 (Active)', 'PF-1 (Reactive)', 'PF-2 (Reactive)']\n",
    "    \n",
    "    \n",
    "    # Create a heatmap of the correlation matrix with seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, vmin=-1, vmax=1, center=0, cmap='coolwarm',\n",
    "                annot=False, fmt='.2f', square=True, xticklabels=all_feature_labels, yticklabels=all_feature_labels)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(f'All Features Correlation Matrix for {v}, {k}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/corr_matrix_{v}_{k}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Create a heatmap of the correlation matrix with seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(ap_features_corr_matrix, vmin=-1, vmax=1, center=0, cmap='coolwarm',\n",
    "                annot=False, fmt='.2f', square=True, xticklabels=active_feature_labels, yticklabels=active_feature_labels)\n",
    "\n",
    "    plt.title(f'Active Power Features Correlation Matrix for {v}, {k}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/AP_corr_matrix_{v}_{k}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(rp_features_corr_matrix, vmin=-1, vmax=1, center=0, cmap='coolwarm',\n",
    "                annot=False, fmt='.2f', square=True, xticklabels=reactive_feature_labels, yticklabels=reactive_feature_labels)\n",
    "    plt.title(f'Reactive Power Features Correlation Matrix for {v}, {k}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/RP_corr_matrix_{v}_{k}.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='pink'> Extract features using IFEEL (maybe pursue later) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dataframes['Gloucester lane tx 1']\n",
    "\n",
    "# Drop all unneeded columns\n",
    "df_test = df_test[['Active Power [kW]', 'Datetime']]\n",
    "\n",
    "# Drop last row from df_test\n",
    "df_test = df_test[:-1]\n",
    "\n",
    "# create a new column with the date only\n",
    "df_test['Date'] = df_test['Datetime'].dt.date\n",
    "\n",
    "# create a new column with the hour of the day\n",
    "df_test['Hour'] = df_test['Datetime'].dt.hour\n",
    "\n",
    "# identify duplicate rows based on \"Date\" and \"Hour\" columns\n",
    "duplicates = df_test[df_test.duplicated(['Date', 'Hour'], keep=False)]\n",
    "# Drop duplicates\n",
    "df_test = df_test.drop_duplicates(['Date', 'Hour'])\n",
    "\n",
    "#df_test.drop_duplicates(subset=['Date', 'Hour'], keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Transform data from long to wide, so that date is the index and each column is a different time\n",
    "\n",
    "\n",
    "\n",
    "df_pivot = df_test.pivot(index = 'Date', columns= 'Hour', values = 'Active Power [kW]')\n",
    "\n",
    "# display the result\n",
    "# convert integer columns to string\n",
    "df_pivot.columns = df_pivot.columns.astype(str)\n",
    "# convert columns to strings with format '%H:%M:%S'\n",
    "df_pivot.columns = [str(hour) + ':00:00' if len(hour) > 1 else '0' + str(hour) + ':00:00' for hour in df_pivot.columns ]\n",
    "\n",
    "df_pivot.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration case, which shows how to use IFEEL to extract interpretable features of electricity loads.\n",
    "# The test dataset at different time intervals can be downloaded from https://github.com/chacehoo/IFEEL/tree/main/Test_Data\n",
    "# The downloaded datasets need to be placed under the current working directory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Read the downloaded data\n",
    "# # df_test = pd.read_csv(\"IFEEL_test_data_1month_1hour.csv\", header=0,index_col=0, parse_dates=False \n",
    " \n",
    " #df_test = pd.read_csv(\"IFEEL/Test_Data/IFEEL_test_data_1month_30mins.csv\", header=0,index_col=0, parse_dates=False)\n",
    "# # df_test = pd.read_csv(\"IFEEL_test_data_1month_2hours.csv\", header=0,index_col=0, parse_dates=False)\n",
    "\n",
    "\n",
    "from IFEEL import ifeel_transformation, ifeel_extraction\n",
    "\n",
    "\n",
    "# You can check the names of all global and peak-period features here.\n",
    "feature_name_global = ifeel_extraction.feature_name_global\n",
    "feature_name_peak = ifeel_extraction.feature_name_peak\n",
    "\n",
    "df_test = dataframes['Gloucester lane tx 1']\n",
    "\n",
    "# Drop all unneeded columns\n",
    "df_test = df_test[['Active Power [kW]', 'Datetime']]\n",
    "\n",
    "# Drop last row from df_test\n",
    "df_test = df_test[:-1]\n",
    "\n",
    "# create a new column with the date only\n",
    "df_test['Date'] = df_test['Datetime'].dt.date\n",
    "\n",
    "# create a new column with the hour of the day\n",
    "df_test['Hour'] = df_test['Datetime'].dt.hour\n",
    "\n",
    "# identify duplicate rows based on \"Date\" and \"Hour\" columns\n",
    "duplicates = df_test[df_test.duplicated(['Date', 'Hour'], keep=False)]\n",
    "# Drop duplicates\n",
    "df_test = df_test.drop_duplicates(['Date', 'Hour'])\n",
    "\n",
    "#df_test.drop_duplicates(subset=['Date', 'Hour'], keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Transform data from long to wide, so that date is the index and each column is a different time\n",
    "df_pivot = df_test.pivot(index = 'Date', columns= 'Hour', values = 'Active Power [kW]')\n",
    "\n",
    "# display the result\n",
    "# convert integer columns to string\n",
    "df_pivot.columns = df_pivot.columns.astype(str)\n",
    "\n",
    "# convert columns to strings with format '%H:%M:%S'\n",
    "df_pivot.columns = [str(hour) + ':00:00' if len(hour) > 1 else '0' + str(hour) + ':00:00' for hour in df_pivot.columns ]\n",
    "\n",
    "\n",
    "\n",
    "sample_interval_in_hour = 24/df_pivot.shape[1]\n",
    "# note: the value of sample interval is in the unit of hour, e.g., if the interval is 30 mins, then sample_interval = 0.5.\n",
    "\n",
    "# Parameter setting\n",
    "# Business hours here are from 9 am to 5 pm\n",
    "time_business_start = 9\n",
    "time_business_end = 17\n",
    "alphabet_size = 7    # alphabet size of SAX representation\n",
    "\n",
    "# Data transformation\n",
    "[df_raw, df_raw_diff, df_SAX_number, df_SAX_alphabet, df_SAX_number_diff] = ifeel_transformation.feature_transformation(df_pivot, alphabet_size,time_business_start,time_business_end)\n",
    "\n",
    "\n",
    "# Global feature extraction for each daily profile\n",
    "feature_global_all_days = pd.DataFrame()\n",
    "for i in np.arange(0, df_raw.shape[0]):\n",
    "    ts = df_raw.iloc[i]\n",
    "    ts_diff = df_raw_diff.iloc[i]\n",
    "    feature_global_all_each = ifeel_extraction.feature_global(ts, ts_diff, sample_interval_in_hour).global_all().T\n",
    "    feature_global_all_days = feature_global_all_days.append(feature_global_all_each, ignore_index=True)\n",
    "\n",
    "feature_global_all_days.columns = ifeel_extraction.feature_name_global\n",
    "feature_global_all_days.head()\n",
    "\n",
    "# Peak feature extraction for each daily profile\n",
    "feature_peak_period_all_days = pd.DataFrame()\n",
    "for i in np.arange(0, df_raw.shape[0]):\n",
    "    ts_sax = df_SAX_number.iloc[i]\n",
    "    ts_sax_diff = df_SAX_number_diff.iloc[i]\n",
    "    feature_peak_all_each = ifeel_extraction.feature_peak_period(ts_sax, ts_sax_diff,alphabet_size, sample_interval_in_hour).T\n",
    "    feature_peak_period_all_days = feature_peak_period_all_days.append(feature_peak_all_each, ignore_index=True)\n",
    "\n",
    "feature_peak_period_all_days.columns = ifeel_extraction.feature_name_peak\n",
    "feature_global_all_days.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'lightgreen'> PCA + Clustering <font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'lightgreen'>  FUNCTION: Print and plot PCA results for each dataset to determine number of components to cluster on </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_plot(df):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(df)\n",
    "\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "\n",
    "    plt.figure(figsize = (5,4))\n",
    "    plt.plot(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')\n",
    "    plt.title('Explained Variance by Components')\n",
    "    plt.xlabel('Number of Components*')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.ylim(0,1.0)\n",
    "    plt.show()\n",
    "for key in df_dict:\n",
    "    print(key)\n",
    "    df = df_dict[key].set_index('substation')\n",
    "    pca_plot(df)\n",
    "    \n",
    "    # rule of thumb, keep 80% of the variacne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'lightgreen'>  FUNCTION: Get pca_scores and cluster labels <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_pca(df, n_components):\n",
    "    # We choose three components. 3 or 4 seems the right choice according to the previous graph.\n",
    "    pca = PCA(n_components = n_components)\n",
    "    #Fit the model the our data with the selected number of components. In our case three.\n",
    "    pca.fit(df)\n",
    "\n",
    "    pca.transform(df)\n",
    "    scores_pca = pca.transform(df)\n",
    "    \n",
    "    return scores_pca\n",
    "\n",
    "def cluster_on_pca_scores(df, scores_pca, n_clusters: int):\n",
    "        # We have chosen four clusters, so we run K-means with number of clusters equals four.\n",
    "    # Same initializer and random state as before.\n",
    "    kmeans_pca = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 42)\n",
    "    # We fit our data with the k-means pa model\n",
    "    kmeans_pca.fit(scores_pca)\n",
    "    # We create a new data frame with the original features and add the PC scores and assigned clusters.\n",
    "    df_segm_pca_kmeans = pd.concat([df.reset_index(drop = True), pd.DataFrame(scores_pca)], axis = 1)\n",
    "    df_segm_pca_kmeans.columns.values[-3: ] = ['Component 1','Component 2', 'Component 3']\n",
    "    # The last column we add contains the pea k-means clustering labels.\n",
    "    df_segm_pca_kmeans['Segment K-means PCA'] = kmeans_pca.labels_\n",
    "\n",
    "    return df_segm_pca_kmeans\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'lightgreen'> Implementation: Cluster on Principle Component Scores <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cluster_results = {}\n",
    "n_components = 3\n",
    "range_of_k_vals = range(2,11)\n",
    "for key in df_dict.keys():\n",
    "    \n",
    "    # Put substation in index so that PCA can work (doesn't work with strings)\n",
    "    df = df_dict[key].set_index('substation')\n",
    "    # Plot PCA results to determine number of components to keep \n",
    "    pca_plot(df)\n",
    "    # Get PCA scores\n",
    "    scores_pca = get_scores_pca(df, n_components)\n",
    "    \n",
    "    # Initialize the dictionary entry for this season/time of week combo \n",
    "    pca_cluster_results[key] = df\n",
    "    \n",
    "    # Run K-means on PCA scores for different numbers of clusters \n",
    "    for n_clusters in range_of_k_vals:\n",
    "        # Get clustering results\n",
    "        these_results = cluster_on_pca_scores(df, scores_pca, n_clusters)\n",
    "        # Add results to dataframe\n",
    "        pca_cluster_results[key][f'Segment K-means PCA_k={n_clusters}'] = np.array(these_results['Segment K-means PCA'])\n",
    "    \n",
    "# Write dataframes to CSV \n",
    "for key in pca_cluster_results:\n",
    "    df_to_write = pca_cluster_results[key].loc[:,pca_cluster_results[key].columns.str.contains('Segment K-means PCA', case=False)]\n",
    "    df_to_write.to_csv(f'pca_cluster_assignments/' + str(key) + '_pca_cluster.csv')\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='lightgreen'> K Means Clustering (w/o PCA) + Analysis </font> <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='lightgreen'> Function: K-Means Clustering  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def k_means(df, k_values = range(2,11), active_only = False):\n",
    "    \n",
    "#     if active_only:\n",
    "#         # Get the feature set - only active power features\n",
    "#         X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "#     else:\n",
    "#         # Get the feature set - all features\n",
    "#         X = df.loc[:, df.columns != 'substation'].values\n",
    "\n",
    "\n",
    "#     # Create an empty list to hold the silhouette scores\n",
    "#     silhouette_scores = []\n",
    "    \n",
    "#     # Loop through each value of k\n",
    "#     for k in k_values:\n",
    "#         # Fit the k-means model to the feature set\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#         labels = kmeans.fit_predict(X)\n",
    "            \n",
    "#         # Calculate the silhouette score for the clustering\n",
    "#         score = silhouette_score(X, labels)\n",
    "#         silhouette_scores.append(score)\n",
    "        \n",
    "#         # Calculate the elbow cost for the clustering\n",
    "        \n",
    "        \n",
    "#         # Assign cluster labels to substations\n",
    "#         df['cluster_{}'.format(k)] = kmeans.labels_\n",
    "\n",
    "#     # # Assign cluster labels to substations\n",
    "#     #     substation_clusters = {}\n",
    "#     #     for substation in df['substation']:\n",
    "#     #         substation_clusters[substation] = kmeans.labels_[i]\n",
    "\n",
    "#     # Plot the silhouette scores for each k value\n",
    "#     plt.plot(silhouette_scores)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Save the cluster assignments \n",
    "#    # all_substation_clusters[k] = substation_clusters\n",
    "    \n",
    "#     return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(df, k_values = range(2,11), active_only = False):\n",
    "    \n",
    "    if active_only:\n",
    "        # Get the feature set - only active power features\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        # Get the feature set - all features\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "\n",
    "    # Create an empty list to hold the silhouette scores and DBI and elbow cost\n",
    "    silhouette_scores = []\n",
    "    dbi_scores = []\n",
    "    wss = []\n",
    "    \n",
    "    # Create empty dict to hold results\n",
    "    results = {}\n",
    "    \n",
    "    # Create an empty dictionary to hold the data points cloest to the center\n",
    "    cluster_centers = {}\n",
    "    \n",
    "    # Loop through each value of k\n",
    "    for k in k_values:\n",
    "        # Fit the k-means model to the feature set\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        # Calculate the silhouette score and DBI for the clustering\n",
    "        score = silhouette_score(X, labels)\n",
    "        db_index = davies_bouldin_score(X, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        dbi_scores.append(db_index)\n",
    "        \n",
    "        # Assign cluster labels to substations\n",
    "        df[f'cluster_{k}'] = kmeans.labels_\n",
    "        \n",
    "        # Calculate the elbow cost for the clustering\n",
    "        kmeans.fit(X)\n",
    "        wss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Save clustering results\n",
    "        results[k] = labels  \n",
    "        \n",
    "       # closest_points = []\n",
    "       # \n",
    "       # centers = np.array(labels.cluster_centers_)    \n",
    "            \n",
    "        # for i in range(k):\n",
    "        #     cluster_center = kmeans.cluster_centers_[i]\n",
    "        #     distances = ((X - cluster_center) ** 2).sum(axis=1)\n",
    "        #     closest_point_idx = np.argmin(distances)\n",
    "        #     closest_point = df.iloc[closest_point_idx]\n",
    "        #     closest_points.append(closest_point['substation'])\n",
    "        # cluster_centers[k] = closest_points\n",
    "\n",
    "    # Plot elbow curve\n",
    "    plt.plot(k_values, wss)\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('WSS')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the silhouette scores and DBI for each k value\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10,10))\n",
    "    ax[0].plot(k_values, silhouette_scores)\n",
    "    ax[0].set_xlabel('Number of clusters (k)')\n",
    "    ax[0].set_ylabel('Silhouette score')\n",
    "    ax[0].set_title('Silhouette score for k-means clustering')\n",
    "    ax[1].plot(k_values, dbi_scores)\n",
    "    ax[1].set_xlabel('Number of clusters (k)')\n",
    "    ax[1].set_ylabel('DBI')\n",
    "    ax[1].set_title('Davies Bouldin Index for k-means clustering')\n",
    "    plt.show()\n",
    "    \n",
    "    return df, results#, cluster_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def k_means_v2(df, k_values = range(2,11), active_only = False):\n",
    "    \n",
    "    if active_only:\n",
    "        # Get the feature set - only active power features\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        # Get the feature set - all features\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "\n",
    "    # Create an empty list to hold the silhouette scores and DBI and elbow cost\n",
    "    silhouette_scores = []\n",
    "    dbi_scores = []\n",
    "    wss = []\n",
    "    \n",
    "    # Create empty dict to hold results\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each value of k\n",
    "    for k in k_values:\n",
    "        # Fit the k-means model to the feature set\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        # Calculate the silhouette score and DBI for the clustering\n",
    "        score = silhouette_score(X, labels)\n",
    "        db_index = davies_bouldin_score(X, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        dbi_scores.append(db_index)\n",
    "        \n",
    "        # Assign cluster labels to substations\n",
    "        df[f'cluster_{k}'] = kmeans.labels_\n",
    "        \n",
    "\n",
    "    # Plot elbow curve\n",
    "    plt.plot(k_values, wss)\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('WSS')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the silhouette scores and DBI for each k value\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10,10))\n",
    "    ax[0].plot(k_values, silhouette_scores)\n",
    "    ax[0].set_xlabel('Number of clusters (k)')\n",
    "    ax[0].set_ylabel('Silhouette score')\n",
    "    ax[0].set_title('Silhouette score for k-means clustering')\n",
    "    ax[1].plot(k_values, dbi_scores)\n",
    "    ax[1].set_xlabel('Number of clusters (k)')\n",
    "    ax[1].set_ylabel('DBI')\n",
    "    ax[1].set_title('Davies Bouldin Index for k-means clustering')\n",
    "    plt.show()\n",
    "    \n",
    "    return df, results\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='lightgreen'> Implementation: K-Means Clustering \n",
    "\n",
    "Decide cluster number (k), present results \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_dict = df_dict.copy()\n",
    "kmeans_results = {}\n",
    "kmeans_cluster_centers = {}\n",
    "for feature_set in [False]:\n",
    "    count = 0\n",
    "    kmeans_results[feature_set] = {}\n",
    "    kmeans_cluster_centers[feature_set] = {}\n",
    "    for k,v in kmeans_dict:\n",
    "        print(k,v)\n",
    "        kmeans_dict[k,v], kmeans_results[feature_set][k,v] = k_means(df_dict[k,v], active_only = feature_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "for k,v in kmeans_dict:\n",
    "    to_write = kmeans_dict[k,v].loc[:, kmeans_dict[k,v].columns.str.contains('cluster|substation')]\n",
    "    to_write.to_csv(f'cluster_assignments/' + str(k) + '_' + str(v) + '_kmeans_cluster.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'lightgreen'> Get list of the cluster centers for the optimal k for a particular time/week combo <font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='lightgreen'> Get feature statistics by cluster </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold the relevant values of the features for each cluster for each season/time of week combo\n",
    "df_stats_numeric = {}\n",
    "df_stats_string = {}\n",
    "\n",
    "# Choose k to work with \n",
    "k = 5\n",
    "\n",
    "# Loop through df_dict \n",
    "for key in df_dict:\n",
    "        \n",
    "    # Group by chosen cluster number\n",
    "    df_grouped = df_dict[key].groupby(f'cluster_{k}')\n",
    "    \n",
    "    # Calculate the mean of each feature for each cluster for all columns that have Active Power and Reactive Power in them\n",
    "    df_mean = df_grouped.agg('mean')\n",
    "    \n",
    "    df_mode = df_grouped.agg(pd.Series.mode)\n",
    "    \n",
    "    # Save stats to dictionary\n",
    "    df_stats_numeric[key] = df_mean\n",
    "    df_stats_string[key] = df_mode\n",
    "    \n",
    "    # Write to csv (first filtering out the unwated columns)\n",
    "    df_mean_to_write = round(df_mean.loc[:,~df_mean.columns.str.contains('cluster')],2)\n",
    "    df_mean_to_write.to_csv(f'example_feature_sets/cluster_{k}_numeric_vals_{key[0]}_{key[1]}.csv')\n",
    "    \n",
    "    df_mode_to_write = round(df_mode.loc[:,~df_mode.columns.str.contains('cluster')],2)\n",
    "    df_mode_to_write = df_mode.loc[:,~df_mode.columns.str.contains('substation')] \n",
    "    df_mode_to_write = df_mode.loc[:,~df_mode.columns.str.contains('Percent')] \n",
    "    df_mode_to_write.to_csv(f'example_feature_sets/cluster_{k}_string_vals_{key[0]}_{key[1]}.csv')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='lightgreen'> Get representative sample from each cluster <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go thru all the season and time of week dataset combos\n",
    "# In each one, Filter the dataset down to each cluster one at a time (range(1,num_clust+1))\n",
    "# Pick a random substation from each cluster\n",
    "\n",
    "num_clust = 5\n",
    "\n",
    "# Initialize dictionary to hold the filtered dataframes\n",
    "representative_substations = {}\n",
    "\n",
    "for k,v in df_dict: \n",
    "    representative_substations[k,v] = {}\n",
    "    cluster_reps = []\n",
    "    for cluster in range(0,num_clust):\n",
    "        # Filter down to the cluster\n",
    "        df = df_dict[k,v].loc[df_dict[k,v][f'cluster_{num_clust}'] == cluster]\n",
    "        # Pick a random substation\n",
    "        substation = df.sample(1)['substation'].values[0]\n",
    "        # Filter down to the substation\n",
    "        #this_cluster_rep = df.loc[df['substation'] == substation]\n",
    "        # Append to the list of cluster reps (appends in order of cluster number)\n",
    "        cluster_reps.append(substation)\n",
    "    print(cluster_reps)\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Save the df to a dictionary\n",
    "    representative_substations[k,v] = cluster_reps\n",
    "        # Save the df to a csv\n",
    "      #  df.to_csv(f'example_feature_sets/cluster_{num_clust}_substation_{substation}_{k}_{v}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_substations['weekend','summer'], representative_substations['week', 'winter']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the median load profiles (or example load profile) for the representative substations\n",
    "<font color = 'orange'> with error bars <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clust = 5\n",
    "for key in representative_substations:\n",
    "    if key == ('weekend', 'summer') or key == ('week', 'winter'):\n",
    "        print(key)\n",
    "        cluster_reps = representative_substations[key]\n",
    "        \n",
    "        for cluster_number, substation in enumerate(cluster_reps):\n",
    "            print(cluster_number, substation)\n",
    "            dataframes[substation]['Hour'] = dataframes[substation].Datetime.dt.hour\n",
    "            fig, ax = plt.subplots(2)\n",
    "            # Main trendlines\n",
    "            ax[0].plot(dataframes[substation].groupby('Hour').agg('Active Power [kW]').median(), color = 'blue')\n",
    "          #  ax[0].plot(dataframes[substation].groupby('Hour').agg('Active Power [kW]').mean(), color = 'blue')\n",
    "            ax[1].plot(dataframes[substation].groupby('Hour').agg('Reactive Power [kVAr]').median(), color = 'orange')\n",
    "          #  ax[1].plot(dataframes[substation].groupby('Hour').agg('Reactive Power [kVAr]').mean(), color = 'green')\n",
    "            \n",
    "            # Error bars\n",
    "            # ax[0].plot(dataframes[substation].groupby('Hour').agg('Active Power [kW]').max(), color = 'black', linestyle = 'dashed', alpha = 0.5)\n",
    "            # ax[0].plot(dataframes[substation].groupby('Hour').agg('Active Power [kW]').min(), color = 'black', linestyle = 'dashed', alpha = 0.5)\n",
    "            # ax[1].plot(dataframes[substation].groupby('Hour').agg('Reactive Power [kVAr]').max(), color = 'black', linestyle = 'dashed', alpha = 0.5)\n",
    "            # ax[1].plot(dataframes[substation].groupby('Hour').agg('Reactive Power [kVAr]').min(), color = 'black', linestyle = 'dashed', alpha = 0.5)\n",
    "            \n",
    "            # Title and Legend\n",
    "            ax[0].set_title(f'Active and Reactive Power Load Profiles for cluster {cluster_number} in {key[0]}, {key[1]} \\n {substation}')\n",
    "            ax[0].legend(labels= ['Median Active Power', 'Mean Active Power'])\n",
    "            ax[1].legend(labels= ['Median Reactive Power', 'Mean Reactive Power'])\n",
    "            \n",
    "            # Axis labels\n",
    "            ax[0].set_xlabel('Hour of the Day')\n",
    "            ax[0].set_ylabel('kW')\n",
    "            ax[1].set_xlabel('Hour of the Day')\n",
    "            ax[1].set_ylabel('kVAr')\n",
    "            \n",
    "            # Save figure\n",
    "            os.makedirs(f'example_load_profiles/{key[0]}_{key[1]}', exist_ok = True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'example_load_profiles/{key[0]}_{key[1]}/rep_ss_load_for_{cluster_number}_(out_of_{num_clust}_clusters).png')\n",
    "            plt.show()\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "##### PSEUDO CODE #####\n",
    "# Go to the representative substation for each cluster and look at the time series data for that substation, from the 'dataframes' dictionary\n",
    "# Then get the average value of active power in each hour of the day for that substation\n",
    "# Plot that... \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'orange'> Considering a specific number of clusters, show the extracted features for the representative substation in each cluster <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'orange'> Analyze Clustering Results <font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal K for each based on elbow method:\n",
    "\n",
    "Just active power\n",
    "\n",
    "_Spring_\n",
    "week: 4, 5 \n",
    "weekend: 3,4,5\n",
    "\n",
    "_Summer_\n",
    "week: 4,5,6\n",
    "weekend: 3,4,5\n",
    "\n",
    "_Fall_\n",
    "week: 3,4,5\n",
    "weekend: 3,4,5\n",
    "\n",
    "_Winter_\n",
    "week: 3,4,5,6\n",
    "weekend: 3,4,5\n",
    "\n",
    "\n",
    "Active and reactive power\n",
    "\n",
    "_Spring_\n",
    "week: 3,5\n",
    "weekend: 4,6\n",
    "\n",
    "_Summer_\n",
    "week: 4,6\n",
    "weekend: 3,4,5\n",
    "\n",
    "_Fall_\n",
    "week: 3,4,5\n",
    "weekend: 2,5\n",
    "\n",
    "_Winter_\n",
    "week: 4,6,7\n",
    "weekend: 3,4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'red' > Fuzzy C- Means (not needed) <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "def fuzzy_c_means(df, c_values = range(2,11), active_only = False, m=2.0, error=0.005, maxiter=1000):\n",
    "    if active_only:\n",
    "        # Get the feature set - only active power features\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        # Get the feature set - all features\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "\n",
    "    # Create an empty dict to hold cluster assignments for each c value\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each value of c\n",
    "    for c in c_values:\n",
    "        # Apply fuzzy c-means clustering to the feature set\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X.T, c, m, error=error, maxiter=maxiter, init=None)\n",
    "        # Calculate the cluster labels based on the maximum degree of membership\n",
    "        labels = np.argmax(u, axis=0)\n",
    "        # Assign cluster labels to substations\n",
    "        df['cluster_{}'.format(c)] = labels\n",
    "        # Save clustering results\n",
    "        results[c] = labels\n",
    "    \n",
    "    return df, results\n",
    "\n",
    "def fuzzy_c_means(df, c_values=range(2, 11), active_only=False, m=2, error=0.005, maxiter=1000):\n",
    "    if active_only:\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "    \n",
    "    X = X.astype(float)  # Convert X to float dtype\n",
    "    \n",
    "    results = {}\n",
    "    for c in c_values:\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X.T, c, m, error=error, maxiter=maxiter, init=None)\n",
    "        labels = np.argmax(u, axis=0)\n",
    "        df['cluster_{}'.format(c)] = labels\n",
    "        results[c] = labels\n",
    "        \n",
    "    return df, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmeans_results = {}\n",
    "for feature_set in [True, False]:\n",
    "    count = 0\n",
    "    cmeans_results[feature_set] = {}\n",
    "    for k,v in df_dict:\n",
    "        df_dict[k,v], cmeans_results[feature_set][k,v] = fuzzy_c_means(df_dict[k,v], active_only = feature_set)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'lightgreen'> GMM Clustering <Font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gmm_clustering(df, n_components_range=range(1, 11), active_only=False):\n",
    "    \n",
    "    if active_only:\n",
    "        # Get the feature set - only active power features\n",
    "        X = df.loc[:, df.columns.str.contains('Active Power')].values\n",
    "    else:\n",
    "        # Get the feature set - all features\n",
    "        X = df.loc[:, df.columns != 'substation'].values\n",
    "\n",
    "    # Create an empty list to hold the BIC scores\n",
    "    bic_scores = []\n",
    "    \n",
    "    # Create empty dict to hold results\n",
    "    results = {}\n",
    "    \n",
    "    # Loop through each value of n_components\n",
    "    for n_components in n_components_range:\n",
    "        # Fit the GMM model to the feature set\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        gmm.fit(X)\n",
    "        # Calculate the BIC for the clustering\n",
    "        bic_scores.append(gmm.bic(X))\n",
    "        \n",
    "        # Save clustering results\n",
    "        results[n_components] = gmm.predict(X)\n",
    "        \n",
    "        # Assign cluster labels to substations\n",
    "         # Assign cluster labels to substations\n",
    "        df[f'gmm_cluster_{n_components}'] = gmm.predict(X)\n",
    "        \n",
    "    # Plot the BIC scores for each n_components value\n",
    "    plt.plot(n_components_range, bic_scores)\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('BIC')\n",
    "    plt.title('BIC for Gaussian Mixture Models')\n",
    "    plt.show()\n",
    "    \n",
    "    return df, results, bic_scores.index(min(bic_scores)) +1 # +1 because index starts at 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_results = {}\n",
    "bic_scores={}\n",
    "gmm_dict = df_dict.copy()\n",
    "for feature_set in [True, False]:\n",
    "    count = 0\n",
    "    gmm_results[feature_set] = {}\n",
    "    bic_scores[feature_set]={}\n",
    "    for k,v in gmm_dict:\n",
    "        print(k,v)\n",
    "        gmm_dict[k,v], gmm_results[feature_set][k,v], bic_scores[feature_set][k,v] = gmm_clustering(df_dict[k,v], active_only = feature_set)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_dict[k,v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = True\n",
    "for k,v in results[feature_set]:\n",
    "    print(k,v )\n",
    "    print(optimal_bic)\n",
    "    optimal_bic = bic_scores[feature_set][k,v]\n",
    "    this_df=  pd.DataFrame(gmm_dict[k,v])\n",
    "    this_df = this_df.loc[:, this_df.columns.str.contains(f'gmm_cluster_{optimal_bic}|substation')]\n",
    "    this_df.to_csv(f'gmm_results/{k}_{v}_gmm.csv', index=False)\n",
    "    \n",
    "  #  pd.DataFrame(results[feature_set][k,v][optimal_bic]).to_csv(f'gmm_results/{k}_{v}_gmm.csv', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='white'> Results </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'orange'> Following https://reader.elsevier.com/reader/sd/pii/S0378779619301993?token=5FAC54D7A6CA6790BEC7580137A4522FCA7111B46B1DD72577956217757743B81C723D768184FB895C91D11C7B83E9B9&originRegion=eu-west-1&originCreation=20230221170442 <font>\n",
    "\n",
    "Cluster substations (e.g. k = 3 as detrmined by dbi), then plot the centroid patterns and the distribution of loads around them (i.e. Fig 6 in the paper) to check if the clusterrs mke sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'orange'> Following https://reader.elsevier.com/reader/sd/pii/S2210670721006533?token=C94DBE9E60982E7F2BF69C1E92E0F2A14398D874B711E87DD5E17BE10DC975D570A63352F06692BB73F5FF6BAF41F582&originRegion=eu-west-1&originCreation=20230221170713 <font>\n",
    "\n",
    "Cluster substations (e.g. k = 3 as detrmined by dbi), then plot the centroid patterns and the distribution of loads around them (i.e. Fig 6 in the paper) to check if the clusterrs mke sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding cluster results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal K for each based on elbow method:\n",
    "\n",
    "Just active power\n",
    "\n",
    "_Spring_ <br>\n",
    "week: 4, 5 \n",
    "weekend: 3,4,5\n",
    "\n",
    "_Summer_ <br>\n",
    "week: 4,5,6\n",
    "weekend: 3,4,5\n",
    "\n",
    "_Fall_ <br>\n",
    "week: 3,4,5\n",
    "weekend: 3,4,5\n",
    "\n",
    "_Winter_ <br>\n",
    "week: 3,4,5,6\n",
    "weekend: 3,4,5\n",
    "\n",
    "\n",
    "Active and reactive power\n",
    "\n",
    "_Spring_ <br>\n",
    "week: 3,5\n",
    "weekend: 4,6\n",
    "\n",
    "_Summer_ <br>\n",
    "week: 4,6\n",
    "weekend: 3,4,5\n",
    "\n",
    "_Fall_ <br>\n",
    "week: 3,4,5\n",
    "weekend: 2,5\n",
    "\n",
    "_Winter_ <br>\n",
    "week: 4,6,7\n",
    "weekend: 3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in results[True]:\n",
    "    print(k,v, results[True][k,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in results[True]:\n",
    "    cluster_assignments = results[True][k,v]\n",
    "    df = pd.DataFrame(cluster_assignments)\n",
    "    df.to_csv(f'cluster_assignments/{k}_{v}.csv')\n",
    "#     for num_clust, labels in cluster_assignments.items():\n",
    "#         # Create dataframe where each column has the cluster assignments for each k value \n",
    "#         df[f'cluster_{num_clust}'] = labels\n",
    "        \n",
    "#         df = pd.DataFrame(labels, columns = ['labels']).sort_values(by ='labels')\n",
    "\n",
    "\n",
    "# df.to_csv(f'cluster_assignments/{k}_{v}_numclust=_{num_clust}.csv')\n",
    "#        # cluster_assignments.sort_values(by=['cluster_6']).to_csv(f'cluster_assignments/{k}_{v}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 5\n",
    "for k,v in df_dict:\n",
    "    print(k,v)\n",
    "  #  df_dict[k,v][]\n",
    "    cluster_assignments = df_dict[k,v][['substation', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8', 'cluster_9', 'cluster_10']]\n",
    "   # print(np.array(cluster_assignments.sort_values(by=['cluster_6'])))\n",
    "  #  cluster_assignments.sort_values(by=['cluster_6']).to_csv(f'cluster_assignments/{k}_{v}.csv')\n",
    "    cluster_assignments.to_csv(f'cluster_assignments/{k}_{v}.csv')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'pink'> pk prd features (not used) </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak period features </br>\n",
    "\n",
    "No. of peak periods  </br>\n",
    "Occurrence time (starting time) of each peak period </br>\n",
    "Shortest time interval between peaks if more than one peak exists  </br>\n",
    "Duration of each peak  </br>\n",
    "Occurrence time of longest peak period  </br>\n",
    "Duration longest peak period  </br>\n",
    "Upward slope of longest peak  </br>\n",
    "Downward slope of the longest peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_peak_period_features(data, alphabet_size=3, window_size=30, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Identifies peak period features in a time series using Symbolic Aggregate Approximation (SAX).\n",
    "    \n",
    "    Parameters:\n",
    "        - data: a list or numpy array containing the time series data\n",
    "        - alphabet_size: the number of symbols to use in the SAX representation (default is 3)\n",
    "        - window_size: the size of the sliding window to use (default is 30)\n",
    "        - threshold: the threshold value used to identify peak periods (default is 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        - A list of tuples, where each tuple contains the start and end index of a peak period.\n",
    "    \"\"\"\n",
    "    \n",
    "    sax = SAX(wordSize=window_size//alphabet_size, alphabetSize=alphabet_size) # Initialize the SAX object\n",
    "    \n",
    "    # Transform the time series data into a SAX representation\n",
    "    sax_data = sax.to_letter_rep(data)\n",
    "    \n",
    "    # Compute the frequencies of each symbol in the SAX representation\n",
    "    freqs = [sax_data.count(sym) / len(sax_data) for sym in sax.get_symbols()]\n",
    "    \n",
    "    # Find the threshold value for identifying peak periods\n",
    "    mean_freq = sum(freqs) / len(freqs)\n",
    "    std_dev_freq = (sum((f - mean_freq)**2 for f in freqs) / len(freqs))**0.5\n",
    "    peak_threshold = mean_freq + std_dev_freq * threshold\n",
    "    \n",
    "    # Find the indices of the start and end of each peak period\n",
    "    peak_periods = []\n",
    "    in_peak_period = False\n",
    "    for i in range(len(sax_data)):\n",
    "        if freqs[sax_data[i]] >= peak_threshold:\n",
    "            if not in_peak_period:\n",
    "                peak_start = i\n",
    "                in_peak_period = True\n",
    "        else:\n",
    "            if in_peak_period:\n",
    "                peak_periods.append((peak_start * window_size, i * window_size))\n",
    "                in_peak_period = False\n",
    "    \n",
    "    if in_peak_period: # Handle the case where a peak period continues to the end of the data\n",
    "        peak_periods.append((peak_start * window_size, len(data)))\n",
    "    \n",
    "    return peak_periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.piecewise import SymbolicAggregateApproximation\n",
    "import numpy as np\n",
    "\n",
    "# Create a random time series\n",
    "X = np.array(dataframes['76 Church Road']['Active Power [kW]'])\n",
    "\n",
    "# Define the SAX transformation parameters\n",
    "n_bins = 4\n",
    "strategy = 'uniform'\n",
    "window_size = 20\n",
    "\n",
    "# Create the SAX object and transform the time series\n",
    "sax = SymbolicAggregateApproximation(n_segments=5, alphabet_size_avg=n_bins)\n",
    "X_sax = sax.fit_transform(X.reshape(1, -1))\n",
    "\n",
    "# Print the transformed time series\n",
    "print(X_sax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sax(time_series, alphabet_size, n_segments):\n",
    "    # Helper function to convert the time series into symbolic representations using SAX\n",
    "    # Returns the symbolic representations\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(time_series.reshape(-1, 1))\n",
    "    kmeans = KMeans(n_clusters=alphabet_size, random_state=0).fit(scaled_data)\n",
    "    distances = np.min(cdist(scaled_data, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "    thresholds = np.percentile(distances, np.linspace(0, 100, alphabet_size + 1)[1:-1])\n",
    "    symbolic_representation = np.zeros(len(scaled_data))\n",
    "    for i in range(1, alphabet_size):\n",
    "        symbolic_representation[distances <= thresholds[i-1]] = i\n",
    "    symbolic_representation = np.array_split(symbolic_representation, n_segments)\n",
    "    symbolic_representation = [''.join([str(int(symbol)) for symbol in segment]) for segment in symbolic_representation]\n",
    "    return symbolic_representation\n",
    "\n",
    "# First, we need to group the data by day\n",
    "grouped = substation_df.groupby(by=[substation_df['date'].dt.date])\n",
    "\n",
    "# Next, we need to iterate through each day to convert the time series into symbolic representations using SAX\n",
    "peak_periods = []\n",
    "for date, group in grouped:\n",
    "    symbolic_representation = sax(group['Active Power [kW]'].values, 5, 24)\n",
    "    peak_periods.append({\n",
    "        'date': date,\n",
    "        'symbolic_representation': symbolic_representation,\n",
    "    })\n",
    "\n",
    "# Now we can extract the various peak period features from the symbolic representations\n",
    "number_of_peak_periods = []\n",
    "occurrence_time_of_peaks = []\n",
    "shortest_time_interval_between_peaks = []\n",
    "duration_of_peaks = []\n",
    "occurrence_time_of_longest_peak = []\n",
    "duration_longest_peak = []\n",
    "for period in peak_periods:\n",
    "    peaks = [i for i, symbol in enumerate(period['symbolic_representation']) if symbol.count('1') >= 4]\n",
    "    if peaks:\n",
    "        number_of_peak_periods.append(len(peaks))\n",
    "        occurrence_time_of_peaks.append([substation_df[substation_df['date'].dt.date == period['date']].iloc[peak]['date'] for peak in peaks])\n",
    "        shortest_time_interval_between_peaks.append(min\n",
    "####\n",
    "\n",
    "    shortest_interval = None\n",
    "    for i in range(1, len(peaks)):\n",
    "        interval = peaks[i] - peaks[i-1]\n",
    "        if shortest_interval is None or interval < shortest_interval:\n",
    "            shortest_interval = interval\n",
    "    shortest_time_interval_between_peaks.append(shortest_interval)\n",
    "\n",
    "    peak_durations = []\n",
    "    for peak in peaks:\n",
    "        start = peak\n",
    "        end = peak\n",
    "        while end < len(period['symbolic_representation']) - 1 and period['symbolic_representation'][end + 1] == '1':\n",
    "            end += 1\n",
    "        peak_durations.append(end - start + 1)\n",
    "    duration_of_peaks.append(peak_durations)\n",
    "\n",
    "    longest_peak_duration = 0\n",
    "    longest_peak_occurrence = None\n",
    "    for i, duration in enumerate(peak_durations):\n",
    "        if duration > longest_peak_duration:\n",
    "            longest_peak_duration = duration\n",
    "            longest_peak_occurrence = occurrence_time_of_peaks[-1][i]\n",
    "    occurrence_time_of_longest_peak.append(longest_peak_occurrence)\n",
    "    duration_longest_peak.append(longest_peak_duration)\n",
    "\n",
    "# To find the upward and downward slopes of the longest peak, we need to access the original time series data\n",
    "upward_slope_longest_peak = []\n",
    "downward_slope_longest_peak = []\n",
    "for i, period in enumerate(peak_periods):\n",
    "    date = period['date']\n",
    "    group = substation_df[substation_df['date'].dt.date == date]\n",
    "    longest_peak_start = group[group['date'] == occurrence_time_of_longest_peak[i]].index[0]\n",
    "    longest_peak_end = longest_peak_start + duration_longest_peak[i] - 1\n",
    "    longest_peak = group[longest_peak_start:longest_peak_end+1]['Active Power [kW]'].values\n",
    "    upward_slope = (longest_peak[-1] - longest_peak[0]) / duration_longest_peak[i]\n",
    "    downward_slope = (longest_peak[0] - longest_peak[-1]) / duration_longest_peak[i]\n",
    "    upward_slope_longest_peak.append(upward_slope)\n",
    "    downward_slope_longest_peak.append(downward_slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substation_df = df\n",
    "# def find_peaks(power_series):\n",
    "#     # Helper function to find the peaks in a time series\n",
    "#     # Returns a list of indeces of peak hours, where peak is defined as an hour with higher active power than both the previous and next hour  \n",
    "#     peaks = []\n",
    "#     for i in range(1, len(power_series) - 1):\n",
    "#         if power_series[i] > power_series[i-1] and power_series[i] > power_series[i+1]:\n",
    "#             peaks.append(i)\n",
    "#     return peaks\n",
    "\n",
    "# # Group data by day\n",
    "# grouped = substation_df.groupby(by=[substation_df['Date']])\n",
    "\n",
    "# # Next, we need to iterate through each day to find the peak periods\n",
    "# peak_periods = []\n",
    "# for date, group in grouped:\n",
    "#     peaks = find_peaks(group['Active Power [kW]'].values)\n",
    "#     if peaks:\n",
    "#         peak_periods.append({\n",
    "#             'date': date,\n",
    "#             'peaks': peaks,\n",
    "#         })\n",
    "\n",
    "# # Now we can extract the various features for each day\n",
    "# number_of_peak_periods = [len(period['peaks']) for period in peak_periods]\n",
    "# occurrence_time_of_peaks = [substation_df.iloc[period['peaks'][0]]['Date'] for period in peak_periods]\n",
    "# duration_of_peaks = []\n",
    "# longest_peak_duration = []\n",
    "# longest_peak_start = []\n",
    "# longest_peak_end = []\n",
    "# longest_peak_upward_slope = []\n",
    "# longest_peak_downward_slope = []\n",
    "# for period in peak_periods:\n",
    "#     peaks = period['peaks']\n",
    "#     date = period['date']\n",
    "#     peak_durations = [peaks[i+1] - peaks[i] for i in range(len(peaks) - 1)]\n",
    "#     duration_of_peaks.append(peak_durations)\n",
    "#     if peak_durations:\n",
    "#         longest_peak_index = np.argmax(peak_durations)\n",
    "#         longest_peak_start.append(substation_df.iloc[peaks[longest_peak_index]]['Date'])\n",
    "#         longest_peak_end.append(substation_df.iloc[peaks[longest_peak_index + 1]]['Date'])\n",
    "#         longest_peak_duration.append(peak_durations[longest_peak_index])\n",
    "#         longest_peak_values = group['Active Power [kW]'].iloc[peaks[longest_peak_index]:peaks[longest_peak_index + 1] + 1].values\n",
    "#         longest_peak_upward_slope.append(np.polyfit(range(len(longest_peak_values)), longest_peak_values, 1)[0])\n",
    "#         longest_peak_downward_slope.append(np.polyfit(range(len(longest_peak_values[::-1])), longest_peak_values[::-1], 1)[0])\n",
    "#     else:\n",
    "#         longest_peak_duration.append(None)\n",
    "#         longest_peak_start.append(None)\n",
    "#         longest_peak_end.append(None)\n",
    "#         longest_peak_upward_slope.append(None)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# daily_groups =df.groupby(['Date'])\n",
    "# peak_hour = daily_groups['Active Power [kW]'].idxmax().map(lambda x: substation_data.loc[x, 'Hour'])\n",
    "# peak_hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='orange'> To-Do: </font>\n",
    "0. Drop substations with >50% missing data <font color='lightgreen'> done! </font> <br>\n",
    "0b. Split substation dataframes by season, week, and weekend <font color='lightgreen'> done! </font> </br>\n",
    "0c. Filter out substations that are missing too many days in each dataframe (determine a threshold) <font color='lightgreen'> done! </font> </br>\n",
    "1. Extract active power features - separately on all datasets <font color = 'lightgreen'> done! </font> \n",
    "2. Clustering on active power features <font color = 'lightgreen'> done! </font>\n",
    "3. Extract reactive power features <font color = 'lightgreen'> done!  </font>\n",
    "4. Cluster on active and reactive power features <font color = 'lightgreen'> done!  </font>\n",
    "5. Download busbar data and repeat analysis <font color = 'orange'> in progress  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c4f4a357890f65c724899bbab8dd054c40207398d57642f947969103c8d943f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
